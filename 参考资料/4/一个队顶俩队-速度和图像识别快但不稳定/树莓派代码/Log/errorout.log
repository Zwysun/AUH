VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:40.119596  1037 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:40.121345  1037 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:40.121847  1037 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:40.135622  1037 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:40.136776  1037 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:40.136888  1037 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:40.137403  1037 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:40.138231  1037 layer_factory.hpp:77] Creating layer input
I0605 09:30:40.138295  1037 net.cpp:100] Creating Layer input
I0605 09:30:40.138321  1037 net.cpp:408] input -> data
I0605 09:30:40.138568  1037 net.cpp:150] Setting up input
I0605 09:30:40.138604  1037 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:40.138640  1037 net.cpp:165] Memory required for data: 49152
I0605 09:30:40.138664  1037 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:40.138712  1037 net.cpp:100] Creating Layer Convolution1
I0605 09:30:40.138736  1037 net.cpp:434] Convolution1 <- data
I0605 09:30:40.138768  1037 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:40.143885  1037 net.cpp:150] Setting up Convolution1
I0605 09:30:40.144829  1037 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:40.145118  1037 net.cpp:165] Memory required for data: 337152
I0605 09:30:40.145323  1037 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:40.145516  1037 net.cpp:100] Creating Layer Pooling1
I0605 09:30:40.145552  1037 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:40.145648  1037 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:40.145829  1037 net.cpp:150] Setting up Pooling1
I0605 09:30:40.145926  1037 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:40.145980  1037 net.cpp:165] Memory required for data: 409152
I0605 09:30:40.146008  1037 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:40.146132  1037 net.cpp:100] Creating Layer Convolution2
I0605 09:30:40.146157  1037 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:40.146296  1037 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:40.148497  1037 net.cpp:150] Setting up Convolution2
I0605 09:30:40.148583  1037 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:40.148648  1037 net.cpp:165] Memory required for data: 544352
I0605 09:30:40.148710  1037 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:40.148779  1037 net.cpp:100] Creating Layer Pooling2
I0605 09:30:40.148800  1037 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:40.148833  1037 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:40.148927  1037 net.cpp:150] Setting up Pooling2
I0605 09:30:40.148946  1037 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:40.148977  1037 net.cpp:165] Memory required for data: 578152
I0605 09:30:40.148999  1037 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:40.149040  1037 net.cpp:100] Creating Layer Convolution3
I0605 09:30:40.149058  1037 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:40.149085  1037 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:40.155516  1037 net.cpp:150] Setting up Convolution3
I0605 09:30:40.156424  1037 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:40.156549  1037 net.cpp:165] Memory required for data: 610552
I0605 09:30:40.156653  1037 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:40.156729  1037 net.cpp:100] Creating Layer Pooling3
I0605 09:30:40.156754  1037 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:40.156797  1037 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:40.156924  1037 net.cpp:150] Setting up Pooling3
I0605 09:30:40.156947  1037 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:40.156993  1037 net.cpp:165] Memory required for data: 620552
I0605 09:30:40.157014  1037 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:40.157065  1037 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:40.157096  1037 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:40.157135  1037 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:40.219373  1037 net.cpp:150] Setting up InnerProduct1
I0605 09:30:40.219622  1037 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:40.219725  1037 net.cpp:165] Memory required for data: 622552
I0605 09:30:40.219815  1037 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:40.220172  1037 net.cpp:100] Creating Layer ReLU1
I0605 09:30:40.220244  1037 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:40.220299  1037 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:40.220376  1037 net.cpp:150] Setting up ReLU1
I0605 09:30:40.220396  1037 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:40.220430  1037 net.cpp:165] Memory required for data: 624552
I0605 09:30:40.220453  1037 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:40.220505  1037 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:40.220525  1037 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:40.220558  1037 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:40.220984  1037 net.cpp:150] Setting up InnerProduct2
I0605 09:30:40.221029  1037 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:40.221074  1037 net.cpp:165] Memory required for data: 624604
I0605 09:30:40.221153  1037 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:40.221205  1037 net.cpp:100] Creating Layer Softmax1
I0605 09:30:40.221238  1037 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:40.221274  1037 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:40.221410  1037 net.cpp:150] Setting up Softmax1
I0605 09:30:40.221436  1037 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:40.221477  1037 net.cpp:165] Memory required for data: 624656
I0605 09:30:40.221499  1037 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:40.221525  1037 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:40.221542  1037 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:40.221559  1037 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:40.221577  1037 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:40.221607  1037 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:40.221627  1037 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:40.221647  1037 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:40.221664  1037 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:40.221683  1037 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:40.221701  1037 net.cpp:228] input does not need backward computation.
I0605 09:30:40.221725  1037 net.cpp:270] This network produces output Softmax1
I0605 09:30:40.221808  1037 net.cpp:283] Network initialization done.
I0605 09:30:40.497532  1037 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:40.510449  1037 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:32:07.181985  1037 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:32:07.182117  1037 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:32:07.182142  1037 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:32:07.183815  1037 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:32:07.184010  1037 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:32:07.184039  1037 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:32:07.184407  1037 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:32:07.185439  1037 layer_factory.hpp:77] Creating layer input
I0605 09:32:07.185524  1037 net.cpp:100] Creating Layer input
I0605 09:32:07.185550  1037 net.cpp:408] input -> data
I0605 09:32:07.185629  1037 net.cpp:150] Setting up input
I0605 09:32:07.185648  1037 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:32:07.185679  1037 net.cpp:165] Memory required for data: 49152
I0605 09:32:07.185703  1037 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:32:07.185747  1037 net.cpp:100] Creating Layer Convolution1
I0605 09:32:07.185767  1037 net.cpp:434] Convolution1 <- data
I0605 09:32:07.185796  1037 net.cpp:408] Convolution1 -> Convolution1
I0605 09:32:07.186043  1037 net.cpp:150] Setting up Convolution1
I0605 09:32:07.186081  1037 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:32:07.186136  1037 net.cpp:165] Memory required for data: 337152
I0605 09:32:07.186240  1037 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:32:07.186323  1037 net.cpp:100] Creating Layer Pooling1
I0605 09:32:07.186368  1037 net.cpp:434] Pooling1 <- Convolution1
I0605 09:32:07.186408  1037 net.cpp:408] Pooling1 -> Pooling1
I0605 09:32:07.186514  1037 net.cpp:150] Setting up Pooling1
I0605 09:32:07.186540  1037 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:32:07.186573  1037 net.cpp:165] Memory required for data: 409152
I0605 09:32:07.186605  1037 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:32:07.186666  1037 net.cpp:100] Creating Layer Convolution2
I0605 09:32:07.186692  1037 net.cpp:434] Convolution2 <- Pooling1
I0605 09:32:07.186771  1037 net.cpp:408] Convolution2 -> Convolution2
I0605 09:32:07.188433  1037 net.cpp:150] Setting up Convolution2
I0605 09:32:07.188531  1037 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:32:07.188578  1037 net.cpp:165] Memory required for data: 544352
I0605 09:32:07.188683  1037 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:32:07.188755  1037 net.cpp:100] Creating Layer Pooling2
I0605 09:32:07.188781  1037 net.cpp:434] Pooling2 <- Convolution2
I0605 09:32:07.188818  1037 net.cpp:408] Pooling2 -> Pooling2
I0605 09:32:07.188905  1037 net.cpp:150] Setting up Pooling2
I0605 09:32:07.188930  1037 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:32:07.188958  1037 net.cpp:165] Memory required for data: 578152
I0605 09:32:07.188987  1037 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:32:07.189040  1037 net.cpp:100] Creating Layer Convolution3
I0605 09:32:07.189061  1037 net.cpp:434] Convolution3 <- Pooling2
I0605 09:32:07.189103  1037 net.cpp:408] Convolution3 -> Convolution3
I0605 09:32:07.195767  1037 net.cpp:150] Setting up Convolution3
I0605 09:32:07.195864  1037 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:32:07.195909  1037 net.cpp:165] Memory required for data: 610552
I0605 09:32:07.195996  1037 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:32:07.196063  1037 net.cpp:100] Creating Layer Pooling3
I0605 09:32:07.196099  1037 net.cpp:434] Pooling3 <- Convolution3
I0605 09:32:07.196142  1037 net.cpp:408] Pooling3 -> Pooling3
I0605 09:32:07.196252  1037 net.cpp:150] Setting up Pooling3
I0605 09:32:07.196276  1037 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:32:07.196306  1037 net.cpp:165] Memory required for data: 620552
I0605 09:32:07.196326  1037 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:32:07.196383  1037 net.cpp:100] Creating Layer InnerProduct1
I0605 09:32:07.196406  1037 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:32:07.196441  1037 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:32:07.260071  1037 net.cpp:150] Setting up InnerProduct1
I0605 09:32:07.260282  1037 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:07.260346  1037 net.cpp:165] Memory required for data: 622552
I0605 09:32:07.260434  1037 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:32:07.260516  1037 net.cpp:100] Creating Layer ReLU1
I0605 09:32:07.260543  1037 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:32:07.260596  1037 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:32:07.260670  1037 net.cpp:150] Setting up ReLU1
I0605 09:32:07.260694  1037 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:07.260736  1037 net.cpp:165] Memory required for data: 624552
I0605 09:32:07.260758  1037 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:32:07.260812  1037 net.cpp:100] Creating Layer InnerProduct2
I0605 09:32:07.260848  1037 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:32:07.260887  1037 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:32:07.261390  1037 net.cpp:150] Setting up InnerProduct2
I0605 09:32:07.261510  1037 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:07.261564  1037 net.cpp:165] Memory required for data: 624604
I0605 09:32:07.261673  1037 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:32:07.261755  1037 net.cpp:100] Creating Layer Softmax1
I0605 09:32:07.261785  1037 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:32:07.261829  1037 net.cpp:408] Softmax1 -> Softmax1
I0605 09:32:07.261945  1037 net.cpp:150] Setting up Softmax1
I0605 09:32:07.261981  1037 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:07.262013  1037 net.cpp:165] Memory required for data: 624656
I0605 09:32:07.262040  1037 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:32:07.262063  1037 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:32:07.262096  1037 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:32:07.262115  1037 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:32:07.262135  1037 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:32:07.262154  1037 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:32:07.262176  1037 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:32:07.262194  1037 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:32:07.262224  1037 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:32:07.262245  1037 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:32:07.262264  1037 net.cpp:228] input does not need backward computation.
I0605 09:32:07.262281  1037 net.cpp:270] This network produces output Softmax1
I0605 09:32:07.262326  1037 net.cpp:283] Network initialization done.
I0605 09:32:07.307238  1037 net.cpp:761] Ignoring source layer ImageData1
I0605 09:32:07.319036  1037 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:29:59.296363  1021 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:29:59.296541  1021 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:29:59.296564  1021 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:29:59.319070  1021 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:29:59.319191  1021 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:29:59.319211  1021 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:29:59.319643  1021 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:29:59.320557  1021 layer_factory.hpp:77] Creating layer input
I0605 09:29:59.320700  1021 net.cpp:100] Creating Layer input
I0605 09:29:59.320746  1021 net.cpp:408] input -> data
I0605 09:29:59.321074  1021 net.cpp:150] Setting up input
I0605 09:29:59.321110  1021 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:29:59.321161  1021 net.cpp:165] Memory required for data: 49152
I0605 09:29:59.321187  1021 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:29:59.321245  1021 net.cpp:100] Creating Layer Convolution1
I0605 09:29:59.321270  1021 net.cpp:434] Convolution1 <- data
I0605 09:29:59.321305  1021 net.cpp:408] Convolution1 -> Convolution1
I0605 09:29:59.326422  1021 net.cpp:150] Setting up Convolution1
I0605 09:29:59.326589  1021 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:29:59.326644  1021 net.cpp:165] Memory required for data: 337152
I0605 09:29:59.326737  1021 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:29:59.326812  1021 net.cpp:100] Creating Layer Pooling1
I0605 09:29:59.326838  1021 net.cpp:434] Pooling1 <- Convolution1
I0605 09:29:59.326886  1021 net.cpp:408] Pooling1 -> Pooling1
I0605 09:29:59.327004  1021 net.cpp:150] Setting up Pooling1
I0605 09:29:59.327028  1021 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:29:59.327061  1021 net.cpp:165] Memory required for data: 409152
I0605 09:29:59.327083  1021 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:29:59.327147  1021 net.cpp:100] Creating Layer Convolution2
I0605 09:29:59.327167  1021 net.cpp:434] Convolution2 <- Pooling1
I0605 09:29:59.327203  1021 net.cpp:408] Convolution2 -> Convolution2
I0605 09:29:59.328636  1021 net.cpp:150] Setting up Convolution2
I0605 09:29:59.328713  1021 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:29:59.328763  1021 net.cpp:165] Memory required for data: 544352
I0605 09:29:59.328836  1021 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:29:59.328907  1021 net.cpp:100] Creating Layer Pooling2
I0605 09:29:59.328929  1021 net.cpp:434] Pooling2 <- Convolution2
I0605 09:29:59.328968  1021 net.cpp:408] Pooling2 -> Pooling2
I0605 09:29:59.329064  1021 net.cpp:150] Setting up Pooling2
I0605 09:29:59.329087  1021 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:29:59.329119  1021 net.cpp:165] Memory required for data: 578152
I0605 09:29:59.329141  1021 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:29:59.329203  1021 net.cpp:100] Creating Layer Convolution3
I0605 09:29:59.329226  1021 net.cpp:434] Convolution3 <- Pooling2
I0605 09:29:59.329260  1021 net.cpp:408] Convolution3 -> Convolution3
I0605 09:29:59.339843  1021 net.cpp:150] Setting up Convolution3
I0605 09:29:59.340440  1021 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:29:59.340615  1021 net.cpp:165] Memory required for data: 610552
I0605 09:29:59.341228  1021 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:29:59.341424  1021 net.cpp:100] Creating Layer Pooling3
I0605 09:29:59.341472  1021 net.cpp:434] Pooling3 <- Convolution3
I0605 09:29:59.341749  1021 net.cpp:408] Pooling3 -> Pooling3
I0605 09:29:59.342082  1021 net.cpp:150] Setting up Pooling3
I0605 09:29:59.342180  1021 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:29:59.342232  1021 net.cpp:165] Memory required for data: 620552
I0605 09:29:59.342273  1021 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:29:59.342351  1021 net.cpp:100] Creating Layer InnerProduct1
I0605 09:29:59.342391  1021 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:29:59.342476  1021 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:29:59.407281  1021 net.cpp:150] Setting up InnerProduct1
I0605 09:29:59.408536  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:29:59.409529  1021 net.cpp:165] Memory required for data: 622552
I0605 09:29:59.409706  1021 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:29:59.410073  1021 net.cpp:100] Creating Layer ReLU1
I0605 09:29:59.410145  1021 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:29:59.410209  1021 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:29:59.410295  1021 net.cpp:150] Setting up ReLU1
I0605 09:29:59.410323  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:29:59.410369  1021 net.cpp:165] Memory required for data: 624552
I0605 09:29:59.410396  1021 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:29:59.410464  1021 net.cpp:100] Creating Layer InnerProduct2
I0605 09:29:59.410488  1021 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:29:59.410535  1021 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:29:59.411023  1021 net.cpp:150] Setting up InnerProduct2
I0605 09:29:59.411128  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:29:59.411176  1021 net.cpp:165] Memory required for data: 624604
I0605 09:29:59.411263  1021 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:29:59.411327  1021 net.cpp:100] Creating Layer Softmax1
I0605 09:29:59.411355  1021 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:29:59.411401  1021 net.cpp:408] Softmax1 -> Softmax1
I0605 09:29:59.411588  1021 net.cpp:150] Setting up Softmax1
I0605 09:29:59.411629  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:29:59.411671  1021 net.cpp:165] Memory required for data: 624656
I0605 09:29:59.411706  1021 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:29:59.411738  1021 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:29:59.411758  1021 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:29:59.411778  1021 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:29:59.411798  1021 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:29:59.411823  1021 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:29:59.411841  1021 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:29:59.411865  1021 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:29:59.411886  1021 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:29:59.411909  1021 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:29:59.411931  1021 net.cpp:228] input does not need backward computation.
I0605 09:29:59.411947  1021 net.cpp:270] This network produces output Softmax1
I0605 09:29:59.412050  1021 net.cpp:283] Network initialization done.
I0605 09:29:59.682548  1021 net.cpp:761] Ignoring source layer ImageData1
I0605 09:29:59.689929  1021 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:31:08.887379  1023 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:31:08.887785  1023 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:31:08.887848  1023 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:31:08.902122  1023 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:31:08.902487  1023 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:31:08.902529  1023 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:31:08.902931  1023 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:31:08.913259  1023 layer_factory.hpp:77] Creating layer input
I0605 09:31:08.913347  1023 net.cpp:100] Creating Layer input
I0605 09:31:08.913373  1023 net.cpp:408] input -> data
I0605 09:31:08.913614  1023 net.cpp:150] Setting up input
I0605 09:31:08.913640  1023 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:31:08.913686  1023 net.cpp:165] Memory required for data: 49152
I0605 09:31:08.913709  1023 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:31:08.913761  1023 net.cpp:100] Creating Layer Convolution1
I0605 09:31:08.913780  1023 net.cpp:434] Convolution1 <- data
I0605 09:31:08.913822  1023 net.cpp:408] Convolution1 -> Convolution1
I0605 09:31:08.915979  1023 net.cpp:150] Setting up Convolution1
I0605 09:31:08.916137  1023 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:31:08.916244  1023 net.cpp:165] Memory required for data: 337152
I0605 09:31:08.916369  1023 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:31:08.916482  1023 net.cpp:100] Creating Layer Pooling1
I0605 09:31:08.916512  1023 net.cpp:434] Pooling1 <- Convolution1
I0605 09:31:08.916568  1023 net.cpp:408] Pooling1 -> Pooling1
I0605 09:31:08.916695  1023 net.cpp:150] Setting up Pooling1
I0605 09:31:08.916723  1023 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:31:08.916757  1023 net.cpp:165] Memory required for data: 409152
I0605 09:31:08.916780  1023 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:31:08.916862  1023 net.cpp:100] Creating Layer Convolution2
I0605 09:31:08.916885  1023 net.cpp:434] Convolution2 <- Pooling1
I0605 09:31:08.916940  1023 net.cpp:408] Convolution2 -> Convolution2
I0605 09:31:08.918650  1023 net.cpp:150] Setting up Convolution2
I0605 09:31:08.918856  1023 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:31:08.918936  1023 net.cpp:165] Memory required for data: 544352
I0605 09:31:08.919121  1023 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:31:08.919279  1023 net.cpp:100] Creating Layer Pooling2
I0605 09:31:08.919387  1023 net.cpp:434] Pooling2 <- Convolution2
I0605 09:31:08.919873  1023 net.cpp:408] Pooling2 -> Pooling2
I0605 09:31:08.920254  1023 net.cpp:150] Setting up Pooling2
I0605 09:31:08.920394  1023 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:31:08.920488  1023 net.cpp:165] Memory required for data: 578152
I0605 09:31:08.920521  1023 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:31:08.920644  1023 net.cpp:100] Creating Layer Convolution3
I0605 09:31:08.920694  1023 net.cpp:434] Convolution3 <- Pooling2
I0605 09:31:08.920761  1023 net.cpp:408] Convolution3 -> Convolution3
I0605 09:31:08.940631  1023 net.cpp:150] Setting up Convolution3
I0605 09:31:08.940733  1023 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:31:08.940779  1023 net.cpp:165] Memory required for data: 610552
I0605 09:31:08.940857  1023 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:31:08.940987  1023 net.cpp:100] Creating Layer Pooling3
I0605 09:31:08.941015  1023 net.cpp:434] Pooling3 <- Convolution3
I0605 09:31:08.941090  1023 net.cpp:408] Pooling3 -> Pooling3
I0605 09:31:08.941198  1023 net.cpp:150] Setting up Pooling3
I0605 09:31:08.941220  1023 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:31:08.941246  1023 net.cpp:165] Memory required for data: 620552
I0605 09:31:08.941265  1023 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:31:08.941326  1023 net.cpp:100] Creating Layer InnerProduct1
I0605 09:31:08.941345  1023 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:31:08.941375  1023 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:31:09.009207  1023 net.cpp:150] Setting up InnerProduct1
I0605 09:31:09.009420  1023 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:09.009488  1023 net.cpp:165] Memory required for data: 622552
I0605 09:31:09.009569  1023 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:31:09.009965  1023 net.cpp:100] Creating Layer ReLU1
I0605 09:31:09.010043  1023 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:31:09.010110  1023 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:31:09.010197  1023 net.cpp:150] Setting up ReLU1
I0605 09:31:09.010219  1023 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:09.010265  1023 net.cpp:165] Memory required for data: 624552
I0605 09:31:09.010290  1023 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:31:09.010351  1023 net.cpp:100] Creating Layer InnerProduct2
I0605 09:31:09.010370  1023 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:31:09.010416  1023 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:31:09.010978  1023 net.cpp:150] Setting up InnerProduct2
I0605 09:31:09.011097  1023 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:09.011157  1023 net.cpp:165] Memory required for data: 624604
I0605 09:31:09.011256  1023 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:31:09.011332  1023 net.cpp:100] Creating Layer Softmax1
I0605 09:31:09.011360  1023 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:31:09.011416  1023 net.cpp:408] Softmax1 -> Softmax1
I0605 09:31:09.011567  1023 net.cpp:150] Setting up Softmax1
I0605 09:31:09.011602  1023 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:09.011651  1023 net.cpp:165] Memory required for data: 624656
I0605 09:31:09.011685  1023 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:31:09.011715  1023 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:31:09.011734  1023 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:31:09.011751  1023 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:31:09.011775  1023 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:31:09.011804  1023 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:31:09.011837  1023 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:31:09.011863  1023 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:31:09.011896  1023 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:31:09.011921  1023 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:31:09.011941  1023 net.cpp:228] input does not need backward computation.
I0605 09:31:09.011960  1023 net.cpp:270] This network produces output Softmax1
I0605 09:31:09.012060  1023 net.cpp:283] Network initialization done.
I0605 09:31:09.289218  1023 net.cpp:761] Ignoring source layer ImageData1
I0605 09:31:09.301277  1023 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:37.827910  1020 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:37.828011  1020 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:37.828029  1020 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:37.841501  1020 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:37.841653  1020 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:37.841686  1020 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:37.842099  1020 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:37.843137  1020 layer_factory.hpp:77] Creating layer input
I0605 09:30:37.843327  1020 net.cpp:100] Creating Layer input
I0605 09:30:37.843369  1020 net.cpp:408] input -> data
I0605 09:30:37.843849  1020 net.cpp:150] Setting up input
I0605 09:30:37.843904  1020 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:37.843964  1020 net.cpp:165] Memory required for data: 49152
I0605 09:30:37.844002  1020 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:37.844097  1020 net.cpp:100] Creating Layer Convolution1
I0605 09:30:37.844123  1020 net.cpp:434] Convolution1 <- data
I0605 09:30:37.844166  1020 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:37.848486  1020 net.cpp:150] Setting up Convolution1
I0605 09:30:37.848707  1020 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:37.848774  1020 net.cpp:165] Memory required for data: 337152
I0605 09:30:37.848873  1020 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:37.848948  1020 net.cpp:100] Creating Layer Pooling1
I0605 09:30:37.848974  1020 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:37.849009  1020 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:37.849157  1020 net.cpp:150] Setting up Pooling1
I0605 09:30:37.849197  1020 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:37.849241  1020 net.cpp:165] Memory required for data: 409152
I0605 09:30:37.849262  1020 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:37.849324  1020 net.cpp:100] Creating Layer Convolution2
I0605 09:30:37.849344  1020 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:37.849377  1020 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:37.850881  1020 net.cpp:150] Setting up Convolution2
I0605 09:30:37.850984  1020 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:37.851033  1020 net.cpp:165] Memory required for data: 544352
I0605 09:30:37.851109  1020 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:37.851168  1020 net.cpp:100] Creating Layer Pooling2
I0605 09:30:37.851200  1020 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:37.851240  1020 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:37.851337  1020 net.cpp:150] Setting up Pooling2
I0605 09:30:37.851361  1020 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:37.851392  1020 net.cpp:165] Memory required for data: 578152
I0605 09:30:37.851411  1020 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:37.851474  1020 net.cpp:100] Creating Layer Convolution3
I0605 09:30:37.851493  1020 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:37.851523  1020 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:37.858702  1020 net.cpp:150] Setting up Convolution3
I0605 09:30:37.858811  1020 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:37.858856  1020 net.cpp:165] Memory required for data: 610552
I0605 09:30:37.858933  1020 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:37.858985  1020 net.cpp:100] Creating Layer Pooling3
I0605 09:30:37.859011  1020 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:37.859058  1020 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:37.859195  1020 net.cpp:150] Setting up Pooling3
I0605 09:30:37.859225  1020 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:37.859257  1020 net.cpp:165] Memory required for data: 620552
I0605 09:30:37.859283  1020 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:37.859357  1020 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:37.859378  1020 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:37.859417  1020 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:37.971722  1020 net.cpp:150] Setting up InnerProduct1
I0605 09:30:37.971876  1020 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:37.971937  1020 net.cpp:165] Memory required for data: 622552
I0605 09:30:37.972013  1020 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:37.972345  1020 net.cpp:100] Creating Layer ReLU1
I0605 09:30:37.972404  1020 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:37.972460  1020 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:37.972549  1020 net.cpp:150] Setting up ReLU1
I0605 09:30:37.972576  1020 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:37.972616  1020 net.cpp:165] Memory required for data: 624552
I0605 09:30:37.972641  1020 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:37.972694  1020 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:37.972715  1020 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:37.972751  1020 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:37.973230  1020 net.cpp:150] Setting up InnerProduct2
I0605 09:30:37.973309  1020 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:37.973352  1020 net.cpp:165] Memory required for data: 624604
I0605 09:30:37.973443  1020 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:37.973506  1020 net.cpp:100] Creating Layer Softmax1
I0605 09:30:37.973539  1020 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:37.973592  1020 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:37.973723  1020 net.cpp:150] Setting up Softmax1
I0605 09:30:37.973755  1020 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:37.973798  1020 net.cpp:165] Memory required for data: 624656
I0605 09:30:37.973826  1020 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:37.973856  1020 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:37.973878  1020 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:37.973901  1020 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:37.973923  1020 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:37.973945  1020 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:37.973970  1020 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:37.973992  1020 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:37.974012  1020 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:37.974035  1020 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:37.974057  1020 net.cpp:228] input does not need backward computation.
I0605 09:30:37.974076  1020 net.cpp:270] This network produces output Softmax1
I0605 09:30:37.974185  1020 net.cpp:283] Network initialization done.
I0605 09:30:38.245648  1020 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:38.254819  1020 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:32:16.883136  1026 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:32:16.883456  1026 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:32:16.883497  1026 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:32:16.906320  1026 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:32:16.906445  1026 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:32:16.906471  1026 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:32:16.906759  1026 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:32:16.907496  1026 layer_factory.hpp:77] Creating layer input
I0605 09:32:16.907552  1026 net.cpp:100] Creating Layer input
I0605 09:32:16.907580  1026 net.cpp:408] input -> data
I0605 09:32:16.907953  1026 net.cpp:150] Setting up input
I0605 09:32:16.907991  1026 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:32:16.908032  1026 net.cpp:165] Memory required for data: 49152
I0605 09:32:16.908061  1026 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:32:16.908118  1026 net.cpp:100] Creating Layer Convolution1
I0605 09:32:16.908149  1026 net.cpp:434] Convolution1 <- data
I0605 09:32:16.908186  1026 net.cpp:408] Convolution1 -> Convolution1
I0605 09:32:16.913269  1026 net.cpp:150] Setting up Convolution1
I0605 09:32:16.913372  1026 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:32:16.913419  1026 net.cpp:165] Memory required for data: 337152
I0605 09:32:16.913507  1026 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:32:16.913565  1026 net.cpp:100] Creating Layer Pooling1
I0605 09:32:16.913592  1026 net.cpp:434] Pooling1 <- Convolution1
I0605 09:32:16.913641  1026 net.cpp:408] Pooling1 -> Pooling1
I0605 09:32:16.913749  1026 net.cpp:150] Setting up Pooling1
I0605 09:32:16.913771  1026 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:32:16.913800  1026 net.cpp:165] Memory required for data: 409152
I0605 09:32:16.913821  1026 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:32:16.913880  1026 net.cpp:100] Creating Layer Convolution2
I0605 09:32:16.913903  1026 net.cpp:434] Convolution2 <- Pooling1
I0605 09:32:16.913934  1026 net.cpp:408] Convolution2 -> Convolution2
I0605 09:32:16.915369  1026 net.cpp:150] Setting up Convolution2
I0605 09:32:16.915427  1026 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:32:16.915472  1026 net.cpp:165] Memory required for data: 544352
I0605 09:32:16.915544  1026 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:32:16.915594  1026 net.cpp:100] Creating Layer Pooling2
I0605 09:32:16.915628  1026 net.cpp:434] Pooling2 <- Convolution2
I0605 09:32:16.915663  1026 net.cpp:408] Pooling2 -> Pooling2
I0605 09:32:16.915742  1026 net.cpp:150] Setting up Pooling2
I0605 09:32:16.915766  1026 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:32:16.915794  1026 net.cpp:165] Memory required for data: 578152
I0605 09:32:16.915814  1026 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:32:16.915866  1026 net.cpp:100] Creating Layer Convolution3
I0605 09:32:16.915889  1026 net.cpp:434] Convolution3 <- Pooling2
I0605 09:32:16.915920  1026 net.cpp:408] Convolution3 -> Convolution3
I0605 09:32:16.922914  1026 net.cpp:150] Setting up Convolution3
I0605 09:32:16.923065  1026 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:32:16.923136  1026 net.cpp:165] Memory required for data: 610552
I0605 09:32:16.923249  1026 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:32:16.923323  1026 net.cpp:100] Creating Layer Pooling3
I0605 09:32:16.923365  1026 net.cpp:434] Pooling3 <- Convolution3
I0605 09:32:16.923421  1026 net.cpp:408] Pooling3 -> Pooling3
I0605 09:32:16.923532  1026 net.cpp:150] Setting up Pooling3
I0605 09:32:16.923553  1026 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:32:16.923586  1026 net.cpp:165] Memory required for data: 620552
I0605 09:32:16.923616  1026 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:32:16.923658  1026 net.cpp:100] Creating Layer InnerProduct1
I0605 09:32:16.923681  1026 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:32:16.923717  1026 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:32:16.991390  1026 net.cpp:150] Setting up InnerProduct1
I0605 09:32:16.992575  1026 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:16.993252  1026 net.cpp:165] Memory required for data: 622552
I0605 09:32:16.993361  1026 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:32:16.993759  1026 net.cpp:100] Creating Layer ReLU1
I0605 09:32:16.993806  1026 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:32:16.993849  1026 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:32:16.993916  1026 net.cpp:150] Setting up ReLU1
I0605 09:32:16.993937  1026 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:16.993973  1026 net.cpp:165] Memory required for data: 624552
I0605 09:32:16.994009  1026 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:32:16.994225  1026 net.cpp:100] Creating Layer InnerProduct2
I0605 09:32:16.994256  1026 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:32:16.994292  1026 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:32:16.994761  1026 net.cpp:150] Setting up InnerProduct2
I0605 09:32:16.994797  1026 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:16.994837  1026 net.cpp:165] Memory required for data: 624604
I0605 09:32:16.994915  1026 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:32:16.994957  1026 net.cpp:100] Creating Layer Softmax1
I0605 09:32:16.994987  1026 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:32:16.995023  1026 net.cpp:408] Softmax1 -> Softmax1
I0605 09:32:16.995141  1026 net.cpp:150] Setting up Softmax1
I0605 09:32:16.995168  1026 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:16.995199  1026 net.cpp:165] Memory required for data: 624656
I0605 09:32:16.995226  1026 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:32:16.995263  1026 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:32:16.995285  1026 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:32:16.995304  1026 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:32:16.995326  1026 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:32:16.995347  1026 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:32:16.995378  1026 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:32:16.995399  1026 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:32:16.995419  1026 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:32:16.995442  1026 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:32:16.995465  1026 net.cpp:228] input does not need backward computation.
I0605 09:32:16.995494  1026 net.cpp:270] This network produces output Softmax1
I0605 09:32:16.995599  1026 net.cpp:283] Network initialization done.
I0605 09:32:17.268973  1026 net.cpp:761] Ignoring source layer ImageData1
I0605 09:32:17.276337  1026 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:02.554289  1036 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:02.554523  1036 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:02.554550  1036 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:02.570235  1036 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:02.570425  1036 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:02.570453  1036 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:02.570816  1036 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:02.572032  1036 layer_factory.hpp:77] Creating layer input
I0605 09:30:02.572190  1036 net.cpp:100] Creating Layer input
I0605 09:30:02.572264  1036 net.cpp:408] input -> data
I0605 09:30:02.572753  1036 net.cpp:150] Setting up input
I0605 09:30:02.572830  1036 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:02.572897  1036 net.cpp:165] Memory required for data: 49152
I0605 09:30:02.572937  1036 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:02.573050  1036 net.cpp:100] Creating Layer Convolution1
I0605 09:30:02.573076  1036 net.cpp:434] Convolution1 <- data
I0605 09:30:02.573127  1036 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:02.577052  1036 net.cpp:150] Setting up Convolution1
I0605 09:30:02.577178  1036 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:02.577226  1036 net.cpp:165] Memory required for data: 337152
I0605 09:30:02.577301  1036 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:02.577370  1036 net.cpp:100] Creating Layer Pooling1
I0605 09:30:02.577394  1036 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:02.577428  1036 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:02.577522  1036 net.cpp:150] Setting up Pooling1
I0605 09:30:02.577546  1036 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:02.577572  1036 net.cpp:165] Memory required for data: 409152
I0605 09:30:02.577592  1036 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:02.577637  1036 net.cpp:100] Creating Layer Convolution2
I0605 09:30:02.577654  1036 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:02.577683  1036 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:02.578876  1036 net.cpp:150] Setting up Convolution2
I0605 09:30:02.578919  1036 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:02.578956  1036 net.cpp:165] Memory required for data: 544352
I0605 09:30:02.579007  1036 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:02.579056  1036 net.cpp:100] Creating Layer Pooling2
I0605 09:30:02.579077  1036 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:02.579107  1036 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:02.579164  1036 net.cpp:150] Setting up Pooling2
I0605 09:30:02.579182  1036 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:02.579206  1036 net.cpp:165] Memory required for data: 578152
I0605 09:30:02.579222  1036 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:02.579262  1036 net.cpp:100] Creating Layer Convolution3
I0605 09:30:02.579278  1036 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:02.579305  1036 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:02.593801  1036 net.cpp:150] Setting up Convolution3
I0605 09:30:02.593997  1036 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:02.594067  1036 net.cpp:165] Memory required for data: 610552
I0605 09:30:02.594167  1036 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:02.594240  1036 net.cpp:100] Creating Layer Pooling3
I0605 09:30:02.594277  1036 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:02.594344  1036 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:02.594498  1036 net.cpp:150] Setting up Pooling3
I0605 09:30:02.594528  1036 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:02.594561  1036 net.cpp:165] Memory required for data: 620552
I0605 09:30:02.594584  1036 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:02.594746  1036 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:02.594774  1036 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:02.594831  1036 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:02.649952  1036 net.cpp:150] Setting up InnerProduct1
I0605 09:30:02.650140  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:02.650207  1036 net.cpp:165] Memory required for data: 622552
I0605 09:30:02.650282  1036 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:02.650660  1036 net.cpp:100] Creating Layer ReLU1
I0605 09:30:02.650732  1036 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:02.650790  1036 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:02.650882  1036 net.cpp:150] Setting up ReLU1
I0605 09:30:02.650907  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:02.650966  1036 net.cpp:165] Memory required for data: 624552
I0605 09:30:02.650997  1036 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:02.651072  1036 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:02.651103  1036 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:02.651160  1036 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:02.651708  1036 net.cpp:150] Setting up InnerProduct2
I0605 09:30:02.651834  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:02.651899  1036 net.cpp:165] Memory required for data: 624604
I0605 09:30:02.651996  1036 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:02.652062  1036 net.cpp:100] Creating Layer Softmax1
I0605 09:30:02.652096  1036 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:02.652148  1036 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:02.652256  1036 net.cpp:150] Setting up Softmax1
I0605 09:30:02.652277  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:02.652312  1036 net.cpp:165] Memory required for data: 624656
I0605 09:30:02.652344  1036 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:02.652374  1036 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:02.652395  1036 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:02.652420  1036 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:02.652439  1036 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:02.652467  1036 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:02.652490  1036 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:02.652511  1036 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:02.652532  1036 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:02.652561  1036 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:02.652586  1036 net.cpp:228] input does not need backward computation.
I0605 09:30:02.652606  1036 net.cpp:270] This network produces output Softmax1
I0605 09:30:02.652737  1036 net.cpp:283] Network initialization done.
I0605 09:30:02.941705  1036 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:02.958326  1036 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:24.667165  1035 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:24.667295  1035 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:24.667315  1035 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:24.683043  1035 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:24.683213  1035 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:24.683236  1035 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:24.683543  1035 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:24.684216  1035 layer_factory.hpp:77] Creating layer input
I0605 09:30:24.684283  1035 net.cpp:100] Creating Layer input
I0605 09:30:24.684314  1035 net.cpp:408] input -> data
I0605 09:30:24.684576  1035 net.cpp:150] Setting up input
I0605 09:30:24.684604  1035 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:24.684645  1035 net.cpp:165] Memory required for data: 49152
I0605 09:30:24.684669  1035 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:24.684720  1035 net.cpp:100] Creating Layer Convolution1
I0605 09:30:24.684741  1035 net.cpp:434] Convolution1 <- data
I0605 09:30:24.684778  1035 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:24.690059  1035 net.cpp:150] Setting up Convolution1
I0605 09:30:24.690156  1035 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:24.690199  1035 net.cpp:165] Memory required for data: 337152
I0605 09:30:24.690277  1035 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:24.690336  1035 net.cpp:100] Creating Layer Pooling1
I0605 09:30:24.690359  1035 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:24.690394  1035 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:24.690482  1035 net.cpp:150] Setting up Pooling1
I0605 09:30:24.690505  1035 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:24.690536  1035 net.cpp:165] Memory required for data: 409152
I0605 09:30:24.690555  1035 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:24.690608  1035 net.cpp:100] Creating Layer Convolution2
I0605 09:30:24.690626  1035 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:24.690656  1035 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:24.692210  1035 net.cpp:150] Setting up Convolution2
I0605 09:30:24.692327  1035 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:24.692366  1035 net.cpp:165] Memory required for data: 544352
I0605 09:30:24.692425  1035 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:24.692477  1035 net.cpp:100] Creating Layer Pooling2
I0605 09:30:24.692502  1035 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:24.692538  1035 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:24.692605  1035 net.cpp:150] Setting up Pooling2
I0605 09:30:24.692626  1035 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:24.692654  1035 net.cpp:165] Memory required for data: 578152
I0605 09:30:24.692672  1035 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:24.692715  1035 net.cpp:100] Creating Layer Convolution3
I0605 09:30:24.692735  1035 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:24.692764  1035 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:24.700620  1035 net.cpp:150] Setting up Convolution3
I0605 09:30:24.702198  1035 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:24.702399  1035 net.cpp:165] Memory required for data: 610552
I0605 09:30:24.702529  1035 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:24.702641  1035 net.cpp:100] Creating Layer Pooling3
I0605 09:30:24.702677  1035 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:24.702754  1035 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:24.702970  1035 net.cpp:150] Setting up Pooling3
I0605 09:30:24.703056  1035 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:24.703097  1035 net.cpp:165] Memory required for data: 620552
I0605 09:30:24.703164  1035 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:24.703215  1035 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:24.703248  1035 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:24.703284  1035 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:24.788677  1035 net.cpp:150] Setting up InnerProduct1
I0605 09:30:24.793879  1035 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:24.794982  1035 net.cpp:165] Memory required for data: 622552
I0605 09:30:24.795694  1035 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:24.796525  1035 net.cpp:100] Creating Layer ReLU1
I0605 09:30:24.797281  1035 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:24.798849  1035 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:24.800346  1035 net.cpp:150] Setting up ReLU1
I0605 09:30:24.801355  1035 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:24.802069  1035 net.cpp:165] Memory required for data: 624552
I0605 09:30:24.802151  1035 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:24.802217  1035 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:24.802248  1035 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:24.802294  1035 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:24.802778  1035 net.cpp:150] Setting up InnerProduct2
I0605 09:30:24.802816  1035 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:24.802855  1035 net.cpp:165] Memory required for data: 624604
I0605 09:30:24.802958  1035 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:24.803028  1035 net.cpp:100] Creating Layer Softmax1
I0605 09:30:24.803051  1035 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:24.803089  1035 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:24.803164  1035 net.cpp:150] Setting up Softmax1
I0605 09:30:24.803184  1035 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:24.803212  1035 net.cpp:165] Memory required for data: 624656
I0605 09:30:24.803236  1035 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:24.803261  1035 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:24.803282  1035 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:24.803300  1035 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:24.803319  1035 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:24.803340  1035 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:24.803360  1035 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:24.803390  1035 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:24.803411  1035 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:24.803431  1035 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:24.803452  1035 net.cpp:228] input does not need backward computation.
I0605 09:30:24.803468  1035 net.cpp:270] This network produces output Softmax1
I0605 09:30:24.803555  1035 net.cpp:283] Network initialization done.
I0605 09:30:25.083483  1035 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:25.090523  1035 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:29:59.478111  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:29:59.478229  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:29:59.478245  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:29:59.492110  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:29:59.492259  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:29:59.492281  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:29:59.492563  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:29:59.493240  1019 layer_factory.hpp:77] Creating layer input
I0605 09:29:59.493345  1019 net.cpp:100] Creating Layer input
I0605 09:29:59.493384  1019 net.cpp:408] input -> data
I0605 09:29:59.493675  1019 net.cpp:150] Setting up input
I0605 09:29:59.493706  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:29:59.493759  1019 net.cpp:165] Memory required for data: 49152
I0605 09:29:59.493784  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:29:59.493845  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:29:59.493871  1019 net.cpp:434] Convolution1 <- data
I0605 09:29:59.493922  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:29:59.499425  1019 net.cpp:150] Setting up Convolution1
I0605 09:29:59.499577  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:29:59.499645  1019 net.cpp:165] Memory required for data: 337152
I0605 09:29:59.499739  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:29:59.499836  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:29:59.499866  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:29:59.499917  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:29:59.500109  1019 net.cpp:150] Setting up Pooling1
I0605 09:29:59.500149  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:29:59.500188  1019 net.cpp:165] Memory required for data: 409152
I0605 09:29:59.500211  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:29:59.500288  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:29:59.500308  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:29:59.500344  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:29:59.501771  1019 net.cpp:150] Setting up Convolution2
I0605 09:29:59.501899  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:29:59.501952  1019 net.cpp:165] Memory required for data: 544352
I0605 09:29:59.502038  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:29:59.502108  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:29:59.502141  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:29:59.502192  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:29:59.502300  1019 net.cpp:150] Setting up Pooling2
I0605 09:29:59.502322  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:29:59.502357  1019 net.cpp:165] Memory required for data: 578152
I0605 09:29:59.502388  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:29:59.502456  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:29:59.502477  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:29:59.502526  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:29:59.510288  1019 net.cpp:150] Setting up Convolution3
I0605 09:29:59.510448  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:29:59.510519  1019 net.cpp:165] Memory required for data: 610552
I0605 09:29:59.510597  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:29:59.510679  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:29:59.510701  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:29:59.510745  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:29:59.510933  1019 net.cpp:150] Setting up Pooling3
I0605 09:29:59.510959  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:29:59.511004  1019 net.cpp:165] Memory required for data: 620552
I0605 09:29:59.511029  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:29:59.511080  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:29:59.511101  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:29:59.511144  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:29:59.588207  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:29:59.588363  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:29:59.588465  1019 net.cpp:165] Memory required for data: 622552
I0605 09:29:59.588546  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:29:59.588837  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:29:59.588920  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:29:59.588964  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:29:59.589049  1019 net.cpp:150] Setting up ReLU1
I0605 09:29:59.589071  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:29:59.589110  1019 net.cpp:165] Memory required for data: 624552
I0605 09:29:59.589151  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:29:59.589200  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:29:59.589221  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:29:59.589290  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:29:59.590174  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:29:59.590296  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:29:59.590340  1019 net.cpp:165] Memory required for data: 624604
I0605 09:29:59.590428  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:29:59.590471  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:29:59.590493  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:29:59.590533  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:29:59.590592  1019 net.cpp:150] Setting up Softmax1
I0605 09:29:59.590610  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:29:59.590638  1019 net.cpp:165] Memory required for data: 624656
I0605 09:29:59.590656  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:29:59.590674  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:29:59.590692  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:29:59.590713  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:29:59.590730  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:29:59.590761  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:29:59.590782  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:29:59.590801  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:29:59.590817  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:29:59.590834  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:29:59.590852  1019 net.cpp:228] input does not need backward computation.
I0605 09:29:59.590867  1019 net.cpp:270] This network produces output Softmax1
I0605 09:29:59.590955  1019 net.cpp:283] Network initialization done.
I0605 09:29:59.866595  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:29:59.882463  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:12.375891  1022 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:12.376097  1022 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:12.376117  1022 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:12.392081  1022 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:12.392396  1022 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:12.392473  1022 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:12.393057  1022 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:12.394191  1022 layer_factory.hpp:77] Creating layer input
I0605 09:30:12.394379  1022 net.cpp:100] Creating Layer input
I0605 09:30:12.394433  1022 net.cpp:408] input -> data
I0605 09:30:12.394896  1022 net.cpp:150] Setting up input
I0605 09:30:12.395000  1022 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:12.395064  1022 net.cpp:165] Memory required for data: 49152
I0605 09:30:12.395105  1022 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:12.395251  1022 net.cpp:100] Creating Layer Convolution1
I0605 09:30:12.395304  1022 net.cpp:434] Convolution1 <- data
I0605 09:30:12.395385  1022 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:12.400229  1022 net.cpp:150] Setting up Convolution1
I0605 09:30:12.401945  1022 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:12.403228  1022 net.cpp:165] Memory required for data: 337152
I0605 09:30:12.404783  1022 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:12.404911  1022 net.cpp:100] Creating Layer Pooling1
I0605 09:30:12.404939  1022 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:12.404978  1022 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:12.405094  1022 net.cpp:150] Setting up Pooling1
I0605 09:30:12.405117  1022 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:12.405148  1022 net.cpp:165] Memory required for data: 409152
I0605 09:30:12.405167  1022 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:12.405222  1022 net.cpp:100] Creating Layer Convolution2
I0605 09:30:12.405241  1022 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:12.405269  1022 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:12.406487  1022 net.cpp:150] Setting up Convolution2
I0605 09:30:12.406523  1022 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:12.406561  1022 net.cpp:165] Memory required for data: 544352
I0605 09:30:12.406613  1022 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:12.406731  1022 net.cpp:100] Creating Layer Pooling2
I0605 09:30:12.406754  1022 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:12.406795  1022 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:12.406870  1022 net.cpp:150] Setting up Pooling2
I0605 09:30:12.406889  1022 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:12.406917  1022 net.cpp:165] Memory required for data: 578152
I0605 09:30:12.406950  1022 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:12.407001  1022 net.cpp:100] Creating Layer Convolution3
I0605 09:30:12.407019  1022 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:12.407049  1022 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:12.416784  1022 net.cpp:150] Setting up Convolution3
I0605 09:30:12.416898  1022 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:12.416968  1022 net.cpp:165] Memory required for data: 610552
I0605 09:30:12.417033  1022 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:12.417102  1022 net.cpp:100] Creating Layer Pooling3
I0605 09:30:12.417124  1022 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:12.417166  1022 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:12.417251  1022 net.cpp:150] Setting up Pooling3
I0605 09:30:12.417269  1022 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:12.417297  1022 net.cpp:165] Memory required for data: 620552
I0605 09:30:12.417328  1022 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:12.417362  1022 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:12.417382  1022 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:12.417414  1022 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:12.486986  1022 net.cpp:150] Setting up InnerProduct1
I0605 09:30:12.487121  1022 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:12.487166  1022 net.cpp:165] Memory required for data: 622552
I0605 09:30:12.487242  1022 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:12.487576  1022 net.cpp:100] Creating Layer ReLU1
I0605 09:30:12.487648  1022 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:12.487711  1022 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:12.487769  1022 net.cpp:150] Setting up ReLU1
I0605 09:30:12.487790  1022 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:12.487834  1022 net.cpp:165] Memory required for data: 624552
I0605 09:30:12.487860  1022 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:12.487907  1022 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:12.487941  1022 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:12.487980  1022 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:12.488453  1022 net.cpp:150] Setting up InnerProduct2
I0605 09:30:12.488523  1022 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:12.488581  1022 net.cpp:165] Memory required for data: 624604
I0605 09:30:12.488663  1022 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:12.488734  1022 net.cpp:100] Creating Layer Softmax1
I0605 09:30:12.488762  1022 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:12.488819  1022 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:12.488981  1022 net.cpp:150] Setting up Softmax1
I0605 09:30:12.489009  1022 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:12.489048  1022 net.cpp:165] Memory required for data: 624656
I0605 09:30:12.489090  1022 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:12.489118  1022 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:12.489136  1022 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:12.489153  1022 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:12.489171  1022 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:12.489202  1022 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:12.489223  1022 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:12.489241  1022 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:12.489261  1022 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:12.489284  1022 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:12.489311  1022 net.cpp:228] input does not need backward computation.
I0605 09:30:12.489328  1022 net.cpp:270] This network produces output Softmax1
I0605 09:30:12.489410  1022 net.cpp:283] Network initialization done.
I0605 09:30:12.770651  1022 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:12.780530  1022 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:32.601096  1021 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:32.601255  1021 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:32.601294  1021 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:32.614846  1021 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:32.614984  1021 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:32.615005  1021 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:32.615478  1021 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:32.617202  1021 layer_factory.hpp:77] Creating layer input
I0605 09:30:32.617599  1021 net.cpp:100] Creating Layer input
I0605 09:30:32.617691  1021 net.cpp:408] input -> data
I0605 09:30:32.618240  1021 net.cpp:150] Setting up input
I0605 09:30:32.618501  1021 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:32.618584  1021 net.cpp:165] Memory required for data: 49152
I0605 09:30:32.618624  1021 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:32.618796  1021 net.cpp:100] Creating Layer Convolution1
I0605 09:30:32.618822  1021 net.cpp:434] Convolution1 <- data
I0605 09:30:32.618881  1021 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:32.622174  1021 net.cpp:150] Setting up Convolution1
I0605 09:30:32.622354  1021 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:32.622413  1021 net.cpp:165] Memory required for data: 337152
I0605 09:30:32.622488  1021 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:32.622561  1021 net.cpp:100] Creating Layer Pooling1
I0605 09:30:32.622586  1021 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:32.622619  1021 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:32.622740  1021 net.cpp:150] Setting up Pooling1
I0605 09:30:32.622776  1021 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:32.622802  1021 net.cpp:165] Memory required for data: 409152
I0605 09:30:32.622822  1021 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:32.622922  1021 net.cpp:100] Creating Layer Convolution2
I0605 09:30:32.622944  1021 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:32.622982  1021 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:32.624608  1021 net.cpp:150] Setting up Convolution2
I0605 09:30:32.624820  1021 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:32.624953  1021 net.cpp:165] Memory required for data: 544352
I0605 09:30:32.625066  1021 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:32.625177  1021 net.cpp:100] Creating Layer Pooling2
I0605 09:30:32.625208  1021 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:32.625258  1021 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:32.625562  1021 net.cpp:150] Setting up Pooling2
I0605 09:30:32.625600  1021 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:32.625667  1021 net.cpp:165] Memory required for data: 578152
I0605 09:30:32.625717  1021 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:32.625859  1021 net.cpp:100] Creating Layer Convolution3
I0605 09:30:32.625927  1021 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:32.625985  1021 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:32.634446  1021 net.cpp:150] Setting up Convolution3
I0605 09:30:32.634696  1021 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:32.634806  1021 net.cpp:165] Memory required for data: 610552
I0605 09:30:32.634945  1021 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:32.635030  1021 net.cpp:100] Creating Layer Pooling3
I0605 09:30:32.635059  1021 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:32.635103  1021 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:32.635231  1021 net.cpp:150] Setting up Pooling3
I0605 09:30:32.635251  1021 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:32.635303  1021 net.cpp:165] Memory required for data: 620552
I0605 09:30:32.635327  1021 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:32.635365  1021 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:32.635382  1021 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:32.635419  1021 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:32.699086  1021 net.cpp:150] Setting up InnerProduct1
I0605 09:30:32.699205  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:32.699244  1021 net.cpp:165] Memory required for data: 622552
I0605 09:30:32.699295  1021 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:32.699488  1021 net.cpp:100] Creating Layer ReLU1
I0605 09:30:32.699514  1021 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:32.699553  1021 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:32.699599  1021 net.cpp:150] Setting up ReLU1
I0605 09:30:32.699617  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:32.699643  1021 net.cpp:165] Memory required for data: 624552
I0605 09:30:32.699662  1021 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:32.699697  1021 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:32.699717  1021 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:32.699743  1021 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:32.700107  1021 net.cpp:150] Setting up InnerProduct2
I0605 09:30:32.700130  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:32.700155  1021 net.cpp:165] Memory required for data: 624604
I0605 09:30:32.700209  1021 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:32.700239  1021 net.cpp:100] Creating Layer Softmax1
I0605 09:30:32.700256  1021 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:32.700281  1021 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:32.700356  1021 net.cpp:150] Setting up Softmax1
I0605 09:30:32.700374  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:32.700397  1021 net.cpp:165] Memory required for data: 624656
I0605 09:30:32.700415  1021 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:32.700435  1021 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:32.700453  1021 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:32.700469  1021 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:32.700484  1021 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:32.700503  1021 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:32.700522  1021 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:32.700541  1021 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:32.700557  1021 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:32.700577  1021 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:32.700593  1021 net.cpp:228] input does not need backward computation.
I0605 09:30:32.700609  1021 net.cpp:270] This network produces output Softmax1
I0605 09:30:32.700665  1021 net.cpp:283] Network initialization done.
I0605 09:30:32.972080  1021 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:32.983685  1021 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:31:46.221786  1021 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:31:46.221916  1021 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:31:46.221935  1021 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:31:46.223604  1021 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:31:46.223752  1021 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:31:46.223790  1021 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:31:46.223991  1021 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:31:46.224865  1021 layer_factory.hpp:77] Creating layer input
I0605 09:31:46.224962  1021 net.cpp:100] Creating Layer input
I0605 09:31:46.224993  1021 net.cpp:408] input -> data
I0605 09:31:46.225086  1021 net.cpp:150] Setting up input
I0605 09:31:46.225107  1021 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:31:46.225162  1021 net.cpp:165] Memory required for data: 49152
I0605 09:31:46.225185  1021 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:31:46.225234  1021 net.cpp:100] Creating Layer Convolution1
I0605 09:31:46.225253  1021 net.cpp:434] Convolution1 <- data
I0605 09:31:46.225307  1021 net.cpp:408] Convolution1 -> Convolution1
I0605 09:31:46.225646  1021 net.cpp:150] Setting up Convolution1
I0605 09:31:46.225819  1021 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:31:46.225881  1021 net.cpp:165] Memory required for data: 337152
I0605 09:31:46.226030  1021 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:31:46.226215  1021 net.cpp:100] Creating Layer Pooling1
I0605 09:31:46.226258  1021 net.cpp:434] Pooling1 <- Convolution1
I0605 09:31:46.226349  1021 net.cpp:408] Pooling1 -> Pooling1
I0605 09:31:46.226495  1021 net.cpp:150] Setting up Pooling1
I0605 09:31:46.226574  1021 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:31:46.226613  1021 net.cpp:165] Memory required for data: 409152
I0605 09:31:46.226665  1021 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:31:46.226730  1021 net.cpp:100] Creating Layer Convolution2
I0605 09:31:46.226753  1021 net.cpp:434] Convolution2 <- Pooling1
I0605 09:31:46.226816  1021 net.cpp:408] Convolution2 -> Convolution2
I0605 09:31:46.228591  1021 net.cpp:150] Setting up Convolution2
I0605 09:31:46.228829  1021 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:31:46.228932  1021 net.cpp:165] Memory required for data: 544352
I0605 09:31:46.229079  1021 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:31:46.229244  1021 net.cpp:100] Creating Layer Pooling2
I0605 09:31:46.229338  1021 net.cpp:434] Pooling2 <- Convolution2
I0605 09:31:46.229432  1021 net.cpp:408] Pooling2 -> Pooling2
I0605 09:31:46.229631  1021 net.cpp:150] Setting up Pooling2
I0605 09:31:46.229718  1021 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:31:46.229815  1021 net.cpp:165] Memory required for data: 578152
I0605 09:31:46.229849  1021 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:31:46.229998  1021 net.cpp:100] Creating Layer Convolution3
I0605 09:31:46.230038  1021 net.cpp:434] Convolution3 <- Pooling2
I0605 09:31:46.230079  1021 net.cpp:408] Convolution3 -> Convolution3
I0605 09:31:46.238485  1021 net.cpp:150] Setting up Convolution3
I0605 09:31:46.238787  1021 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:31:46.238831  1021 net.cpp:165] Memory required for data: 610552
I0605 09:31:46.238993  1021 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:31:46.239063  1021 net.cpp:100] Creating Layer Pooling3
I0605 09:31:46.239084  1021 net.cpp:434] Pooling3 <- Convolution3
I0605 09:31:46.239122  1021 net.cpp:408] Pooling3 -> Pooling3
I0605 09:31:46.239297  1021 net.cpp:150] Setting up Pooling3
I0605 09:31:46.239326  1021 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:31:46.239364  1021 net.cpp:165] Memory required for data: 620552
I0605 09:31:46.239396  1021 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:31:46.239442  1021 net.cpp:100] Creating Layer InnerProduct1
I0605 09:31:46.239460  1021 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:31:46.239497  1021 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:31:46.305933  1021 net.cpp:150] Setting up InnerProduct1
I0605 09:31:46.306078  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:46.306118  1021 net.cpp:165] Memory required for data: 622552
I0605 09:31:46.306181  1021 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:31:46.306229  1021 net.cpp:100] Creating Layer ReLU1
I0605 09:31:46.306252  1021 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:31:46.306303  1021 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:31:46.306344  1021 net.cpp:150] Setting up ReLU1
I0605 09:31:46.306360  1021 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:46.306383  1021 net.cpp:165] Memory required for data: 624552
I0605 09:31:46.306406  1021 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:31:46.306435  1021 net.cpp:100] Creating Layer InnerProduct2
I0605 09:31:46.306452  1021 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:31:46.306478  1021 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:31:46.306864  1021 net.cpp:150] Setting up InnerProduct2
I0605 09:31:46.306938  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:46.306964  1021 net.cpp:165] Memory required for data: 624604
I0605 09:31:46.307019  1021 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:31:46.307046  1021 net.cpp:100] Creating Layer Softmax1
I0605 09:31:46.307065  1021 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:31:46.307090  1021 net.cpp:408] Softmax1 -> Softmax1
I0605 09:31:46.307130  1021 net.cpp:150] Setting up Softmax1
I0605 09:31:46.307198  1021 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:46.307221  1021 net.cpp:165] Memory required for data: 624656
I0605 09:31:46.307238  1021 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:31:46.307257  1021 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:31:46.307289  1021 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:31:46.307307  1021 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:31:46.307322  1021 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:31:46.307340  1021 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:31:46.307358  1021 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:31:46.307376  1021 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:31:46.307400  1021 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:31:46.307417  1021 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:31:46.307436  1021 net.cpp:228] input does not need backward computation.
I0605 09:31:46.307451  1021 net.cpp:270] This network produces output Softmax1
I0605 09:31:46.307488  1021 net.cpp:283] Network initialization done.
I0605 09:31:46.349301  1021 net.cpp:761] Ignoring source layer ImageData1
I0605 09:31:46.356860  1021 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
Traceback (most recent call last):
  File "/home/pi/Desktop/send_receive.py", line 7, in <module>
    from opencv_detect_redline import readframe
ImportError: cannot import name 'readframe'
Traceback (most recent call last):
  File "/home/pi/Desktop/send_receive.py", line 7, in <module>
    from opencv_detect_redline import readframe
ImportError: cannot import name 'readframe'
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:45.669072  1028 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:45.669358  1028 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:45.669384  1028 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:45.684195  1028 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:45.684432  1028 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:45.684471  1028 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:45.684952  1028 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:45.685792  1028 layer_factory.hpp:77] Creating layer input
I0605 09:30:45.685851  1028 net.cpp:100] Creating Layer input
I0605 09:30:45.685880  1028 net.cpp:408] input -> data
I0605 09:30:45.686225  1028 net.cpp:150] Setting up input
I0605 09:30:45.686256  1028 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:45.686295  1028 net.cpp:165] Memory required for data: 49152
I0605 09:30:45.686321  1028 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:45.686386  1028 net.cpp:100] Creating Layer Convolution1
I0605 09:30:45.686409  1028 net.cpp:434] Convolution1 <- data
I0605 09:30:45.686451  1028 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:45.691027  1028 net.cpp:150] Setting up Convolution1
I0605 09:30:45.691140  1028 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:45.691187  1028 net.cpp:165] Memory required for data: 337152
I0605 09:30:45.691272  1028 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:45.691335  1028 net.cpp:100] Creating Layer Pooling1
I0605 09:30:45.691363  1028 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:45.691401  1028 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:45.691510  1028 net.cpp:150] Setting up Pooling1
I0605 09:30:45.691537  1028 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:45.691568  1028 net.cpp:165] Memory required for data: 409152
I0605 09:30:45.691587  1028 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:45.691642  1028 net.cpp:100] Creating Layer Convolution2
I0605 09:30:45.691663  1028 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:45.691697  1028 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:45.693096  1028 net.cpp:150] Setting up Convolution2
I0605 09:30:45.693156  1028 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:45.693197  1028 net.cpp:165] Memory required for data: 544352
I0605 09:30:45.693264  1028 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:45.693320  1028 net.cpp:100] Creating Layer Pooling2
I0605 09:30:45.693344  1028 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:45.693382  1028 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:45.693455  1028 net.cpp:150] Setting up Pooling2
I0605 09:30:45.693478  1028 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:45.693509  1028 net.cpp:165] Memory required for data: 578152
I0605 09:30:45.693528  1028 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:45.693578  1028 net.cpp:100] Creating Layer Convolution3
I0605 09:30:45.693599  1028 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:45.693634  1028 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:45.700261  1028 net.cpp:150] Setting up Convolution3
I0605 09:30:45.700423  1028 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:45.700491  1028 net.cpp:165] Memory required for data: 610552
I0605 09:30:45.700599  1028 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:45.700673  1028 net.cpp:100] Creating Layer Pooling3
I0605 09:30:45.700704  1028 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:45.700757  1028 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:45.700876  1028 net.cpp:150] Setting up Pooling3
I0605 09:30:45.700906  1028 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:45.700942  1028 net.cpp:165] Memory required for data: 620552
I0605 09:30:45.700965  1028 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:45.701014  1028 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:45.701036  1028 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:45.701073  1028 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:45.773202  1028 net.cpp:150] Setting up InnerProduct1
I0605 09:30:45.773352  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:45.773412  1028 net.cpp:165] Memory required for data: 622552
I0605 09:30:45.773483  1028 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:45.773818  1028 net.cpp:100] Creating Layer ReLU1
I0605 09:30:45.773887  1028 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:45.773957  1028 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:45.774039  1028 net.cpp:150] Setting up ReLU1
I0605 09:30:45.774061  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:45.774102  1028 net.cpp:165] Memory required for data: 624552
I0605 09:30:45.774127  1028 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:45.774178  1028 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:45.774199  1028 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:45.774241  1028 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:45.774735  1028 net.cpp:150] Setting up InnerProduct2
I0605 09:30:45.774796  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:45.774847  1028 net.cpp:165] Memory required for data: 624604
I0605 09:30:45.774932  1028 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:45.774998  1028 net.cpp:100] Creating Layer Softmax1
I0605 09:30:45.775025  1028 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:45.775074  1028 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:45.775224  1028 net.cpp:150] Setting up Softmax1
I0605 09:30:45.775251  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:45.775280  1028 net.cpp:165] Memory required for data: 624656
I0605 09:30:45.775301  1028 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:45.775326  1028 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:45.775352  1028 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:45.775370  1028 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:45.775389  1028 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:45.775411  1028 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:45.775432  1028 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:45.775454  1028 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:45.775476  1028 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:45.775507  1028 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:45.775530  1028 net.cpp:228] input does not need backward computation.
I0605 09:30:45.775547  1028 net.cpp:270] This network produces output Softmax1
I0605 09:30:45.775633  1028 net.cpp:283] Network initialization done.
I0605 09:30:46.062458  1028 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:46.073000  1028 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:35:53.095917  1028 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:35:53.096020  1028 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:35:53.096042  1028 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:35:53.096992  1028 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:35:53.097060  1028 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:35:53.097086  1028 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:35:53.097299  1028 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:35:53.097998  1028 layer_factory.hpp:77] Creating layer input
I0605 09:35:53.098053  1028 net.cpp:100] Creating Layer input
I0605 09:35:53.098086  1028 net.cpp:408] input -> data
I0605 09:35:53.098196  1028 net.cpp:150] Setting up input
I0605 09:35:53.098228  1028 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:35:53.098264  1028 net.cpp:165] Memory required for data: 49152
I0605 09:35:53.098289  1028 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:35:53.098342  1028 net.cpp:100] Creating Layer Convolution1
I0605 09:35:53.098366  1028 net.cpp:434] Convolution1 <- data
I0605 09:35:53.098403  1028 net.cpp:408] Convolution1 -> Convolution1
I0605 09:35:53.098757  1028 net.cpp:150] Setting up Convolution1
I0605 09:35:53.098809  1028 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:35:53.098891  1028 net.cpp:165] Memory required for data: 337152
I0605 09:35:53.098986  1028 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:35:53.099056  1028 net.cpp:100] Creating Layer Pooling1
I0605 09:35:53.099083  1028 net.cpp:434] Pooling1 <- Convolution1
I0605 09:35:53.099133  1028 net.cpp:408] Pooling1 -> Pooling1
I0605 09:35:53.099225  1028 net.cpp:150] Setting up Pooling1
I0605 09:35:53.099249  1028 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:35:53.099283  1028 net.cpp:165] Memory required for data: 409152
I0605 09:35:53.099308  1028 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:35:53.099524  1028 net.cpp:100] Creating Layer Convolution2
I0605 09:35:53.099560  1028 net.cpp:434] Convolution2 <- Pooling1
I0605 09:35:53.099613  1028 net.cpp:408] Convolution2 -> Convolution2
I0605 09:35:53.101410  1028 net.cpp:150] Setting up Convolution2
I0605 09:35:53.101544  1028 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:35:53.101606  1028 net.cpp:165] Memory required for data: 544352
I0605 09:35:53.101685  1028 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:35:53.101791  1028 net.cpp:100] Creating Layer Pooling2
I0605 09:35:53.101819  1028 net.cpp:434] Pooling2 <- Convolution2
I0605 09:35:53.101891  1028 net.cpp:408] Pooling2 -> Pooling2
I0605 09:35:53.101990  1028 net.cpp:150] Setting up Pooling2
I0605 09:35:53.102016  1028 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:35:53.102049  1028 net.cpp:165] Memory required for data: 578152
I0605 09:35:53.102072  1028 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:35:53.102139  1028 net.cpp:100] Creating Layer Convolution3
I0605 09:35:53.102161  1028 net.cpp:434] Convolution3 <- Pooling2
I0605 09:35:53.102206  1028 net.cpp:408] Convolution3 -> Convolution3
I0605 09:35:53.109985  1028 net.cpp:150] Setting up Convolution3
I0605 09:35:53.111117  1028 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:35:53.111780  1028 net.cpp:165] Memory required for data: 610552
I0605 09:35:53.112375  1028 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:35:53.112917  1028 net.cpp:100] Creating Layer Pooling3
I0605 09:35:53.113431  1028 net.cpp:434] Pooling3 <- Convolution3
I0605 09:35:53.113906  1028 net.cpp:408] Pooling3 -> Pooling3
I0605 09:35:53.114821  1028 net.cpp:150] Setting up Pooling3
I0605 09:35:53.116154  1028 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:35:53.117308  1028 net.cpp:165] Memory required for data: 620552
I0605 09:35:53.117386  1028 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:35:53.117447  1028 net.cpp:100] Creating Layer InnerProduct1
I0605 09:35:53.117473  1028 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:35:53.117519  1028 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:35:53.180341  1028 net.cpp:150] Setting up InnerProduct1
I0605 09:35:53.180511  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:35:53.180568  1028 net.cpp:165] Memory required for data: 622552
I0605 09:35:53.180639  1028 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:35:53.180711  1028 net.cpp:100] Creating Layer ReLU1
I0605 09:35:53.180743  1028 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:35:53.180795  1028 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:35:53.180871  1028 net.cpp:150] Setting up ReLU1
I0605 09:35:53.180892  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:35:53.180927  1028 net.cpp:165] Memory required for data: 624552
I0605 09:35:53.180949  1028 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:35:53.181004  1028 net.cpp:100] Creating Layer InnerProduct2
I0605 09:35:53.181026  1028 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:35:53.181066  1028 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:35:53.181578  1028 net.cpp:150] Setting up InnerProduct2
I0605 09:35:53.181661  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:35:53.181718  1028 net.cpp:165] Memory required for data: 624604
I0605 09:35:53.181818  1028 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:35:53.181900  1028 net.cpp:100] Creating Layer Softmax1
I0605 09:35:53.181933  1028 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:35:53.182001  1028 net.cpp:408] Softmax1 -> Softmax1
I0605 09:35:53.182106  1028 net.cpp:150] Setting up Softmax1
I0605 09:35:53.182132  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:35:53.182168  1028 net.cpp:165] Memory required for data: 624656
I0605 09:35:53.182198  1028 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:35:53.182237  1028 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:35:53.182263  1028 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:35:53.182286  1028 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:35:53.182307  1028 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:35:53.182332  1028 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:35:53.182359  1028 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:35:53.182381  1028 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:35:53.182407  1028 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:35:53.182435  1028 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:35:53.182461  1028 net.cpp:228] input does not need backward computation.
I0605 09:35:53.182482  1028 net.cpp:270] This network produces output Softmax1
I0605 09:35:53.182541  1028 net.cpp:283] Network initialization done.
I0605 09:35:53.214637  1028 net.cpp:761] Ignoring source layer ImageData1
I0605 09:35:53.222780  1028 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:38:00.424403  1028 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:38:00.424595  1028 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:38:00.424618  1028 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:38:00.426276  1028 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:38:00.426415  1028 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:38:00.426439  1028 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:38:00.426630  1028 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:38:00.432157  1028 layer_factory.hpp:77] Creating layer input
I0605 09:38:00.432252  1028 net.cpp:100] Creating Layer input
I0605 09:38:00.432286  1028 net.cpp:408] input -> data
I0605 09:38:00.432379  1028 net.cpp:150] Setting up input
I0605 09:38:00.432404  1028 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:38:00.432446  1028 net.cpp:165] Memory required for data: 49152
I0605 09:38:00.432480  1028 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:38:00.432531  1028 net.cpp:100] Creating Layer Convolution1
I0605 09:38:00.432552  1028 net.cpp:434] Convolution1 <- data
I0605 09:38:00.432595  1028 net.cpp:408] Convolution1 -> Convolution1
I0605 09:38:00.432898  1028 net.cpp:150] Setting up Convolution1
I0605 09:38:00.432929  1028 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:38:00.432981  1028 net.cpp:165] Memory required for data: 337152
I0605 09:38:00.433040  1028 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:38:00.433092  1028 net.cpp:100] Creating Layer Pooling1
I0605 09:38:00.433116  1028 net.cpp:434] Pooling1 <- Convolution1
I0605 09:38:00.433145  1028 net.cpp:408] Pooling1 -> Pooling1
I0605 09:38:00.433214  1028 net.cpp:150] Setting up Pooling1
I0605 09:38:00.433235  1028 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:38:00.433262  1028 net.cpp:165] Memory required for data: 409152
I0605 09:38:00.433281  1028 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:38:00.433331  1028 net.cpp:100] Creating Layer Convolution2
I0605 09:38:00.433352  1028 net.cpp:434] Convolution2 <- Pooling1
I0605 09:38:00.433382  1028 net.cpp:408] Convolution2 -> Convolution2
I0605 09:38:00.435031  1028 net.cpp:150] Setting up Convolution2
I0605 09:38:00.435124  1028 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:38:00.435168  1028 net.cpp:165] Memory required for data: 544352
I0605 09:38:00.435247  1028 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:38:00.435307  1028 net.cpp:100] Creating Layer Pooling2
I0605 09:38:00.435338  1028 net.cpp:434] Pooling2 <- Convolution2
I0605 09:38:00.435379  1028 net.cpp:408] Pooling2 -> Pooling2
I0605 09:38:00.435479  1028 net.cpp:150] Setting up Pooling2
I0605 09:38:00.435503  1028 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:38:00.435533  1028 net.cpp:165] Memory required for data: 578152
I0605 09:38:00.435552  1028 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:38:00.435614  1028 net.cpp:100] Creating Layer Convolution3
I0605 09:38:00.435634  1028 net.cpp:434] Convolution3 <- Pooling2
I0605 09:38:00.435667  1028 net.cpp:408] Convolution3 -> Convolution3
I0605 09:38:00.443295  1028 net.cpp:150] Setting up Convolution3
I0605 09:38:00.443457  1028 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:38:00.443503  1028 net.cpp:165] Memory required for data: 610552
I0605 09:38:00.443586  1028 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:38:00.443647  1028 net.cpp:100] Creating Layer Pooling3
I0605 09:38:00.443672  1028 net.cpp:434] Pooling3 <- Convolution3
I0605 09:38:00.443756  1028 net.cpp:408] Pooling3 -> Pooling3
I0605 09:38:00.443873  1028 net.cpp:150] Setting up Pooling3
I0605 09:38:00.443894  1028 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:38:00.443924  1028 net.cpp:165] Memory required for data: 620552
I0605 09:38:00.443944  1028 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:38:00.444008  1028 net.cpp:100] Creating Layer InnerProduct1
I0605 09:38:00.444027  1028 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:38:00.444058  1028 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:38:00.522284  1028 net.cpp:150] Setting up InnerProduct1
I0605 09:38:00.522377  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:38:00.522418  1028 net.cpp:165] Memory required for data: 622552
I0605 09:38:00.522473  1028 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:38:00.522527  1028 net.cpp:100] Creating Layer ReLU1
I0605 09:38:00.522552  1028 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:38:00.522593  1028 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:38:00.522644  1028 net.cpp:150] Setting up ReLU1
I0605 09:38:00.522661  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:38:00.522691  1028 net.cpp:165] Memory required for data: 624552
I0605 09:38:00.522712  1028 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:38:00.522749  1028 net.cpp:100] Creating Layer InnerProduct2
I0605 09:38:00.522769  1028 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:38:00.522799  1028 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:38:00.523213  1028 net.cpp:150] Setting up InnerProduct2
I0605 09:38:00.523238  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:38:00.523267  1028 net.cpp:165] Memory required for data: 624604
I0605 09:38:00.523316  1028 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:38:00.523350  1028 net.cpp:100] Creating Layer Softmax1
I0605 09:38:00.523370  1028 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:38:00.523399  1028 net.cpp:408] Softmax1 -> Softmax1
I0605 09:38:00.523443  1028 net.cpp:150] Setting up Softmax1
I0605 09:38:00.523463  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:38:00.523488  1028 net.cpp:165] Memory required for data: 624656
I0605 09:38:00.523509  1028 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:38:00.523530  1028 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:38:00.523550  1028 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:38:00.523567  1028 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:38:00.523587  1028 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:38:00.523608  1028 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:38:00.523628  1028 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:38:00.523648  1028 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:38:00.523667  1028 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:38:00.523687  1028 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:38:00.523707  1028 net.cpp:228] input does not need backward computation.
I0605 09:38:00.523725  1028 net.cpp:270] This network produces output Softmax1
I0605 09:38:00.523766  1028 net.cpp:283] Network initialization done.
I0605 09:38:00.562727  1028 net.cpp:761] Ignoring source layer ImageData1
I0605 09:38:00.576428  1028 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:40:15.730859  1028 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:40:15.730935  1028 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:40:15.730955  1028 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:40:15.731714  1028 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:40:15.731789  1028 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:40:15.731809  1028 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:40:15.731974  1028 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:40:15.732589  1028 layer_factory.hpp:77] Creating layer input
I0605 09:40:15.732640  1028 net.cpp:100] Creating Layer input
I0605 09:40:15.732671  1028 net.cpp:408] input -> data
I0605 09:40:15.732753  1028 net.cpp:150] Setting up input
I0605 09:40:15.732774  1028 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:40:15.732810  1028 net.cpp:165] Memory required for data: 49152
I0605 09:40:15.732836  1028 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:40:15.732882  1028 net.cpp:100] Creating Layer Convolution1
I0605 09:40:15.732903  1028 net.cpp:434] Convolution1 <- data
I0605 09:40:15.732936  1028 net.cpp:408] Convolution1 -> Convolution1
I0605 09:40:15.733160  1028 net.cpp:150] Setting up Convolution1
I0605 09:40:15.733183  1028 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:40:15.733217  1028 net.cpp:165] Memory required for data: 337152
I0605 09:40:15.733268  1028 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:40:15.733306  1028 net.cpp:100] Creating Layer Pooling1
I0605 09:40:15.733327  1028 net.cpp:434] Pooling1 <- Convolution1
I0605 09:40:15.733361  1028 net.cpp:408] Pooling1 -> Pooling1
I0605 09:40:15.733420  1028 net.cpp:150] Setting up Pooling1
I0605 09:40:15.733443  1028 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:40:15.733471  1028 net.cpp:165] Memory required for data: 409152
I0605 09:40:15.733491  1028 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:40:15.733536  1028 net.cpp:100] Creating Layer Convolution2
I0605 09:40:15.733554  1028 net.cpp:434] Convolution2 <- Pooling1
I0605 09:40:15.733587  1028 net.cpp:408] Convolution2 -> Convolution2
I0605 09:40:15.734946  1028 net.cpp:150] Setting up Convolution2
I0605 09:40:15.735010  1028 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:40:15.735049  1028 net.cpp:165] Memory required for data: 544352
I0605 09:40:15.735110  1028 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:40:15.735157  1028 net.cpp:100] Creating Layer Pooling2
I0605 09:40:15.735178  1028 net.cpp:434] Pooling2 <- Convolution2
I0605 09:40:15.735213  1028 net.cpp:408] Pooling2 -> Pooling2
I0605 09:40:15.735275  1028 net.cpp:150] Setting up Pooling2
I0605 09:40:15.735296  1028 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:40:15.735323  1028 net.cpp:165] Memory required for data: 578152
I0605 09:40:15.735342  1028 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:40:15.735384  1028 net.cpp:100] Creating Layer Convolution3
I0605 09:40:15.735404  1028 net.cpp:434] Convolution3 <- Pooling2
I0605 09:40:15.735433  1028 net.cpp:408] Convolution3 -> Convolution3
I0605 09:40:15.742069  1028 net.cpp:150] Setting up Convolution3
I0605 09:40:15.742187  1028 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:40:15.742238  1028 net.cpp:165] Memory required for data: 610552
I0605 09:40:15.742328  1028 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:40:15.742395  1028 net.cpp:100] Creating Layer Pooling3
I0605 09:40:15.742421  1028 net.cpp:434] Pooling3 <- Convolution3
I0605 09:40:15.742472  1028 net.cpp:408] Pooling3 -> Pooling3
I0605 09:40:15.742578  1028 net.cpp:150] Setting up Pooling3
I0605 09:40:15.742602  1028 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:40:15.742641  1028 net.cpp:165] Memory required for data: 620552
I0605 09:40:15.742666  1028 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:40:15.742722  1028 net.cpp:100] Creating Layer InnerProduct1
I0605 09:40:15.742745  1028 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:40:15.742786  1028 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:40:15.831529  1028 net.cpp:150] Setting up InnerProduct1
I0605 09:40:15.832376  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:40:15.832506  1028 net.cpp:165] Memory required for data: 622552
I0605 09:40:15.832593  1028 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:40:15.832672  1028 net.cpp:100] Creating Layer ReLU1
I0605 09:40:15.832718  1028 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:40:15.832762  1028 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:40:15.832849  1028 net.cpp:150] Setting up ReLU1
I0605 09:40:15.832870  1028 net.cpp:157] Top shape: 1 500 (500)
I0605 09:40:15.832897  1028 net.cpp:165] Memory required for data: 624552
I0605 09:40:15.832921  1028 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:40:15.832983  1028 net.cpp:100] Creating Layer InnerProduct2
I0605 09:40:15.833004  1028 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:40:15.833037  1028 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:40:15.833516  1028 net.cpp:150] Setting up InnerProduct2
I0605 09:40:15.833560  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:40:15.833613  1028 net.cpp:165] Memory required for data: 624604
I0605 09:40:15.833689  1028 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:40:15.833760  1028 net.cpp:100] Creating Layer Softmax1
I0605 09:40:15.833784  1028 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:40:15.833845  1028 net.cpp:408] Softmax1 -> Softmax1
I0605 09:40:15.833926  1028 net.cpp:150] Setting up Softmax1
I0605 09:40:15.833968  1028 net.cpp:157] Top shape: 1 13 (13)
I0605 09:40:15.833995  1028 net.cpp:165] Memory required for data: 624656
I0605 09:40:15.834018  1028 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:40:15.834043  1028 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:40:15.834065  1028 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:40:15.834098  1028 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:40:15.834120  1028 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:40:15.834141  1028 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:40:15.834162  1028 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:40:15.834184  1028 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:40:15.834221  1028 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:40:15.834244  1028 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:40:15.834264  1028 net.cpp:228] input does not need backward computation.
I0605 09:40:15.834282  1028 net.cpp:270] This network produces output Softmax1
I0605 09:40:15.834343  1028 net.cpp:283] Network initialization done.
I0605 09:40:15.863396  1028 net.cpp:761] Ignoring source layer ImageData1
I0605 09:40:15.875089  1028 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:32:21.946807  1036 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:32:21.946911  1036 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:32:21.946941  1036 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:32:21.974768  1036 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:32:21.974964  1036 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:32:21.974989  1036 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:32:21.975358  1036 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:32:21.976095  1036 layer_factory.hpp:77] Creating layer input
I0605 09:32:21.976145  1036 net.cpp:100] Creating Layer input
I0605 09:32:21.976194  1036 net.cpp:408] input -> data
I0605 09:32:21.976485  1036 net.cpp:150] Setting up input
I0605 09:32:21.976513  1036 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:32:21.976557  1036 net.cpp:165] Memory required for data: 49152
I0605 09:32:21.976584  1036 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:32:21.976637  1036 net.cpp:100] Creating Layer Convolution1
I0605 09:32:21.976675  1036 net.cpp:434] Convolution1 <- data
I0605 09:32:21.976714  1036 net.cpp:408] Convolution1 -> Convolution1
I0605 09:32:21.981627  1036 net.cpp:150] Setting up Convolution1
I0605 09:32:21.981935  1036 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:32:21.982003  1036 net.cpp:165] Memory required for data: 337152
I0605 09:32:21.982125  1036 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:32:21.982247  1036 net.cpp:100] Creating Layer Pooling1
I0605 09:32:21.982281  1036 net.cpp:434] Pooling1 <- Convolution1
I0605 09:32:21.982551  1036 net.cpp:408] Pooling1 -> Pooling1
I0605 09:32:21.982738  1036 net.cpp:150] Setting up Pooling1
I0605 09:32:21.982766  1036 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:32:21.982838  1036 net.cpp:165] Memory required for data: 409152
I0605 09:32:21.982867  1036 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:32:21.982959  1036 net.cpp:100] Creating Layer Convolution2
I0605 09:32:21.982986  1036 net.cpp:434] Convolution2 <- Pooling1
I0605 09:32:21.983032  1036 net.cpp:408] Convolution2 -> Convolution2
I0605 09:32:21.984803  1036 net.cpp:150] Setting up Convolution2
I0605 09:32:21.984995  1036 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:32:21.985260  1036 net.cpp:165] Memory required for data: 544352
I0605 09:32:21.985381  1036 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:32:21.985468  1036 net.cpp:100] Creating Layer Pooling2
I0605 09:32:21.985498  1036 net.cpp:434] Pooling2 <- Convolution2
I0605 09:32:21.985553  1036 net.cpp:408] Pooling2 -> Pooling2
I0605 09:32:21.985713  1036 net.cpp:150] Setting up Pooling2
I0605 09:32:21.985747  1036 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:32:21.985821  1036 net.cpp:165] Memory required for data: 578152
I0605 09:32:21.985848  1036 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:32:21.985930  1036 net.cpp:100] Creating Layer Convolution3
I0605 09:32:21.985956  1036 net.cpp:434] Convolution3 <- Pooling2
I0605 09:32:21.985999  1036 net.cpp:408] Convolution3 -> Convolution3
I0605 09:32:22.004176  1036 net.cpp:150] Setting up Convolution3
I0605 09:32:22.004279  1036 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:32:22.004348  1036 net.cpp:165] Memory required for data: 610552
I0605 09:32:22.004428  1036 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:32:22.004484  1036 net.cpp:100] Creating Layer Pooling3
I0605 09:32:22.004513  1036 net.cpp:434] Pooling3 <- Convolution3
I0605 09:32:22.004560  1036 net.cpp:408] Pooling3 -> Pooling3
I0605 09:32:22.004640  1036 net.cpp:150] Setting up Pooling3
I0605 09:32:22.004681  1036 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:32:22.004709  1036 net.cpp:165] Memory required for data: 620552
I0605 09:32:22.004731  1036 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:32:22.004770  1036 net.cpp:100] Creating Layer InnerProduct1
I0605 09:32:22.004809  1036 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:32:22.004842  1036 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:32:22.092279  1036 net.cpp:150] Setting up InnerProduct1
I0605 09:32:22.092420  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:22.092469  1036 net.cpp:165] Memory required for data: 622552
I0605 09:32:22.092531  1036 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:32:22.092996  1036 net.cpp:100] Creating Layer ReLU1
I0605 09:32:22.093129  1036 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:32:22.093194  1036 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:32:22.093252  1036 net.cpp:150] Setting up ReLU1
I0605 09:32:22.093272  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:32:22.093320  1036 net.cpp:165] Memory required for data: 624552
I0605 09:32:22.093344  1036 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:32:22.093385  1036 net.cpp:100] Creating Layer InnerProduct2
I0605 09:32:22.093405  1036 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:32:22.093446  1036 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:32:22.093931  1036 net.cpp:150] Setting up InnerProduct2
I0605 09:32:22.093963  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:22.093993  1036 net.cpp:165] Memory required for data: 624604
I0605 09:32:22.094070  1036 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:32:22.094110  1036 net.cpp:100] Creating Layer Softmax1
I0605 09:32:22.094131  1036 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:32:22.094187  1036 net.cpp:408] Softmax1 -> Softmax1
I0605 09:32:22.094278  1036 net.cpp:150] Setting up Softmax1
I0605 09:32:22.094401  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:32:22.094439  1036 net.cpp:165] Memory required for data: 624656
I0605 09:32:22.094463  1036 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:32:22.094491  1036 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:32:22.094511  1036 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:32:22.094530  1036 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:32:22.094558  1036 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:32:22.094578  1036 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:32:22.094600  1036 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:32:22.094620  1036 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:32:22.094641  1036 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:32:22.094678  1036 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:32:22.094703  1036 net.cpp:228] input does not need backward computation.
I0605 09:32:22.094722  1036 net.cpp:270] This network produces output Softmax1
I0605 09:32:22.094810  1036 net.cpp:283] Network initialization done.
I0605 09:32:22.378890  1036 net.cpp:761] Ignoring source layer ImageData1
I0605 09:32:22.389547  1036 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:34:56.708468  1036 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:34:56.708644  1036 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:34:56.708676  1036 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:34:56.709842  1036 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:34:56.709993  1036 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:34:56.710021  1036 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:34:56.710273  1036 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:34:56.711201  1036 layer_factory.hpp:77] Creating layer input
I0605 09:34:56.711290  1036 net.cpp:100] Creating Layer input
I0605 09:34:56.711326  1036 net.cpp:408] input -> data
I0605 09:34:56.711424  1036 net.cpp:150] Setting up input
I0605 09:34:56.711448  1036 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:34:56.711488  1036 net.cpp:165] Memory required for data: 49152
I0605 09:34:56.711515  1036 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:34:56.711577  1036 net.cpp:100] Creating Layer Convolution1
I0605 09:34:56.711598  1036 net.cpp:434] Convolution1 <- data
I0605 09:34:56.711637  1036 net.cpp:408] Convolution1 -> Convolution1
I0605 09:34:56.711880  1036 net.cpp:150] Setting up Convolution1
I0605 09:34:56.711910  1036 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:34:56.711946  1036 net.cpp:165] Memory required for data: 337152
I0605 09:34:56.712008  1036 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:34:56.712072  1036 net.cpp:100] Creating Layer Pooling1
I0605 09:34:56.712097  1036 net.cpp:434] Pooling1 <- Convolution1
I0605 09:34:56.712136  1036 net.cpp:408] Pooling1 -> Pooling1
I0605 09:34:56.712246  1036 net.cpp:150] Setting up Pooling1
I0605 09:34:56.712275  1036 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:34:56.712325  1036 net.cpp:165] Memory required for data: 409152
I0605 09:34:56.712350  1036 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:34:56.712411  1036 net.cpp:100] Creating Layer Convolution2
I0605 09:34:56.712435  1036 net.cpp:434] Convolution2 <- Pooling1
I0605 09:34:56.712481  1036 net.cpp:408] Convolution2 -> Convolution2
I0605 09:34:56.713997  1036 net.cpp:150] Setting up Convolution2
I0605 09:34:56.714112  1036 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:34:56.714160  1036 net.cpp:165] Memory required for data: 544352
I0605 09:34:56.714247  1036 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:34:56.714329  1036 net.cpp:100] Creating Layer Pooling2
I0605 09:34:56.714354  1036 net.cpp:434] Pooling2 <- Convolution2
I0605 09:34:56.714403  1036 net.cpp:408] Pooling2 -> Pooling2
I0605 09:34:56.714520  1036 net.cpp:150] Setting up Pooling2
I0605 09:34:56.714547  1036 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:34:56.714584  1036 net.cpp:165] Memory required for data: 578152
I0605 09:34:56.714609  1036 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:34:56.714675  1036 net.cpp:100] Creating Layer Convolution3
I0605 09:34:56.714699  1036 net.cpp:434] Convolution3 <- Pooling2
I0605 09:34:56.714740  1036 net.cpp:408] Convolution3 -> Convolution3
I0605 09:34:56.722203  1036 net.cpp:150] Setting up Convolution3
I0605 09:34:56.722426  1036 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:34:56.722544  1036 net.cpp:165] Memory required for data: 610552
I0605 09:34:56.722666  1036 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:34:56.722759  1036 net.cpp:100] Creating Layer Pooling3
I0605 09:34:56.722796  1036 net.cpp:434] Pooling3 <- Convolution3
I0605 09:34:56.722841  1036 net.cpp:408] Pooling3 -> Pooling3
I0605 09:34:56.722952  1036 net.cpp:150] Setting up Pooling3
I0605 09:34:56.722982  1036 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:34:56.723028  1036 net.cpp:165] Memory required for data: 620552
I0605 09:34:56.723057  1036 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:34:56.723125  1036 net.cpp:100] Creating Layer InnerProduct1
I0605 09:34:56.723157  1036 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:34:56.723212  1036 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:34:56.786350  1036 net.cpp:150] Setting up InnerProduct1
I0605 09:34:56.786514  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:34:56.786587  1036 net.cpp:165] Memory required for data: 622552
I0605 09:34:56.786674  1036 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:34:56.786767  1036 net.cpp:100] Creating Layer ReLU1
I0605 09:34:56.786803  1036 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:34:56.786857  1036 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:34:56.786952  1036 net.cpp:150] Setting up ReLU1
I0605 09:34:56.786973  1036 net.cpp:157] Top shape: 1 500 (500)
I0605 09:34:56.787009  1036 net.cpp:165] Memory required for data: 624552
I0605 09:34:56.787034  1036 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:34:56.787091  1036 net.cpp:100] Creating Layer InnerProduct2
I0605 09:34:56.787117  1036 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:34:56.787161  1036 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:34:56.787698  1036 net.cpp:150] Setting up InnerProduct2
I0605 09:34:56.787750  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:34:56.787786  1036 net.cpp:165] Memory required for data: 624604
I0605 09:34:56.787848  1036 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:34:56.787892  1036 net.cpp:100] Creating Layer Softmax1
I0605 09:34:56.787920  1036 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:34:56.787961  1036 net.cpp:408] Softmax1 -> Softmax1
I0605 09:34:56.788033  1036 net.cpp:150] Setting up Softmax1
I0605 09:34:56.788053  1036 net.cpp:157] Top shape: 1 13 (13)
I0605 09:34:56.788082  1036 net.cpp:165] Memory required for data: 624656
I0605 09:34:56.788108  1036 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:34:56.788134  1036 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:34:56.788156  1036 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:34:56.788177  1036 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:34:56.788200  1036 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:34:56.788231  1036 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:34:56.788254  1036 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:34:56.788276  1036 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:34:56.788300  1036 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:34:56.788321  1036 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:34:56.788344  1036 net.cpp:228] input does not need backward computation.
I0605 09:34:56.788364  1036 net.cpp:270] This network produces output Softmax1
I0605 09:34:56.788414  1036 net.cpp:283] Network initialization done.
I0605 09:34:56.827028  1036 net.cpp:761] Ignoring source layer ImageData1
I0605 09:34:56.839632  1036 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0605 09:30:19.115803  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:30:19.116055  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:30:19.116086  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:30:19.133399  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:30:19.133574  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:30:19.133601  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:30:19.133982  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:30:19.134914  1019 layer_factory.hpp:77] Creating layer input
I0605 09:30:19.135054  1019 net.cpp:100] Creating Layer input
I0605 09:30:19.135092  1019 net.cpp:408] input -> data
I0605 09:30:19.135372  1019 net.cpp:150] Setting up input
I0605 09:30:19.135408  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:30:19.135455  1019 net.cpp:165] Memory required for data: 49152
I0605 09:30:19.135488  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:30:19.135576  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:30:19.135609  1019 net.cpp:434] Convolution1 <- data
I0605 09:30:19.135661  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:30:19.139935  1019 net.cpp:150] Setting up Convolution1
I0605 09:30:19.140091  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:30:19.140164  1019 net.cpp:165] Memory required for data: 337152
I0605 09:30:19.140267  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:30:19.140362  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:30:19.140396  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:30:19.140451  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:30:19.140581  1019 net.cpp:150] Setting up Pooling1
I0605 09:30:19.140612  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:30:19.140650  1019 net.cpp:165] Memory required for data: 409152
I0605 09:30:19.140674  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:30:19.140738  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:30:19.140760  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:30:19.140794  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:30:19.142213  1019 net.cpp:150] Setting up Convolution2
I0605 09:30:19.142277  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:30:19.142321  1019 net.cpp:165] Memory required for data: 544352
I0605 09:30:19.142393  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:30:19.142447  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:30:19.142472  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:30:19.142513  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:30:19.142587  1019 net.cpp:150] Setting up Pooling2
I0605 09:30:19.142607  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:30:19.142637  1019 net.cpp:165] Memory required for data: 578152
I0605 09:30:19.142655  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:30:19.142702  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:30:19.142721  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:30:19.142752  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:30:19.149340  1019 net.cpp:150] Setting up Convolution3
I0605 09:30:19.149477  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:30:19.149528  1019 net.cpp:165] Memory required for data: 610552
I0605 09:30:19.149618  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:30:19.149695  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:30:19.149726  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:30:19.149777  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:30:19.149899  1019 net.cpp:150] Setting up Pooling3
I0605 09:30:19.149924  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:30:19.149957  1019 net.cpp:165] Memory required for data: 620552
I0605 09:30:19.149978  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:30:19.150029  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:30:19.150053  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:30:19.150089  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:30:19.206094  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:30:19.206218  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:19.206261  1019 net.cpp:165] Memory required for data: 622552
I0605 09:30:19.206320  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:30:19.206539  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:30:19.206568  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:30:19.206604  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:30:19.206663  1019 net.cpp:150] Setting up ReLU1
I0605 09:30:19.206681  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:30:19.206710  1019 net.cpp:165] Memory required for data: 624552
I0605 09:30:19.206730  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:30:19.206770  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:30:19.206789  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:30:19.206816  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:30:19.207190  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:30:19.207216  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:19.207242  1019 net.cpp:165] Memory required for data: 624604
I0605 09:30:19.207295  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:30:19.207324  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:30:19.207342  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:30:19.207370  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:30:19.207415  1019 net.cpp:150] Setting up Softmax1
I0605 09:30:19.207432  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:30:19.207455  1019 net.cpp:165] Memory required for data: 624656
I0605 09:30:19.207473  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:30:19.207492  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:30:19.207510  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:30:19.207528  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:30:19.207545  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:30:19.207563  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:30:19.207581  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:30:19.207598  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:30:19.207617  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:30:19.207635  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:30:19.207653  1019 net.cpp:228] input does not need backward computation.
I0605 09:30:19.207669  1019 net.cpp:270] This network produces output Softmax1
I0605 09:30:19.207737  1019 net.cpp:283] Network initialization done.
I0605 09:30:19.489042  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:30:19.499560  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:31:54.052137  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:31:54.052304  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:31:54.052433  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:31:54.054255  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:31:54.054569  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:31:54.054643  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:31:54.054903  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:31:54.055860  1019 layer_factory.hpp:77] Creating layer input
I0605 09:31:54.056033  1019 net.cpp:100] Creating Layer input
I0605 09:31:54.056073  1019 net.cpp:408] input -> data
I0605 09:31:54.056172  1019 net.cpp:150] Setting up input
I0605 09:31:54.056196  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:31:54.056244  1019 net.cpp:165] Memory required for data: 49152
I0605 09:31:54.056277  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:31:54.056337  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:31:54.056429  1019 net.cpp:434] Convolution1 <- data
I0605 09:31:54.056478  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:31:54.056847  1019 net.cpp:150] Setting up Convolution1
I0605 09:31:54.056907  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:31:54.056948  1019 net.cpp:165] Memory required for data: 337152
I0605 09:31:54.057027  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:31:54.057086  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:31:54.057109  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:31:54.057159  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:31:54.057271  1019 net.cpp:150] Setting up Pooling1
I0605 09:31:54.057297  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:31:54.057337  1019 net.cpp:165] Memory required for data: 409152
I0605 09:31:54.057446  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:31:54.057569  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:31:54.057600  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:31:54.057777  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:31:54.059659  1019 net.cpp:150] Setting up Convolution2
I0605 09:31:54.059722  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:31:54.059774  1019 net.cpp:165] Memory required for data: 544352
I0605 09:31:54.059849  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:31:54.059922  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:31:54.059950  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:31:54.059999  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:31:54.060087  1019 net.cpp:150] Setting up Pooling2
I0605 09:31:54.060127  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:31:54.060159  1019 net.cpp:165] Memory required for data: 578152
I0605 09:31:54.060183  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:31:54.060254  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:31:54.060277  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:31:54.060313  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:31:54.069190  1019 net.cpp:150] Setting up Convolution3
I0605 09:31:54.070283  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:31:54.070477  1019 net.cpp:165] Memory required for data: 610552
I0605 09:31:54.070603  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:31:54.070706  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:31:54.070740  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:31:54.070799  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:31:54.070963  1019 net.cpp:150] Setting up Pooling3
I0605 09:31:54.071002  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:31:54.071054  1019 net.cpp:165] Memory required for data: 620552
I0605 09:31:54.071087  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:31:54.071156  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:31:54.071194  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:31:54.071321  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:31:54.150099  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:31:54.150310  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:54.150447  1019 net.cpp:165] Memory required for data: 622552
I0605 09:31:54.150578  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:31:54.150699  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:31:54.150734  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:31:54.150794  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:31:54.150876  1019 net.cpp:150] Setting up ReLU1
I0605 09:31:54.150900  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:31:54.150936  1019 net.cpp:165] Memory required for data: 624552
I0605 09:31:54.150964  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:31:54.151026  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:31:54.151049  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:31:54.151087  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:31:54.151693  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:31:54.151764  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:54.151803  1019 net.cpp:165] Memory required for data: 624604
I0605 09:31:54.151885  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:31:54.151932  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:31:54.151953  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:31:54.151995  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:31:54.152070  1019 net.cpp:150] Setting up Softmax1
I0605 09:31:54.152088  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:31:54.152124  1019 net.cpp:165] Memory required for data: 624656
I0605 09:31:54.152148  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:31:54.152169  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:31:54.152189  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:31:54.152212  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:31:54.152232  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:31:54.152264  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:31:54.152288  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:31:54.152314  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:31:54.152336  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:31:54.152359  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:31:54.152464  1019 net.cpp:228] input does not need backward computation.
I0605 09:31:54.152484  1019 net.cpp:270] This network produces output Softmax1
I0605 09:31:54.152648  1019 net.cpp:283] Network initialization done.
I0605 09:31:54.197816  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:31:54.208930  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:33:43.548243  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:33:43.548390  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:33:43.548416  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:33:43.549803  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:33:43.549996  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:33:43.550036  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:33:43.550340  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:33:43.551415  1019 layer_factory.hpp:77] Creating layer input
I0605 09:33:43.551599  1019 net.cpp:100] Creating Layer input
I0605 09:33:43.551651  1019 net.cpp:408] input -> data
I0605 09:33:43.551802  1019 net.cpp:150] Setting up input
I0605 09:33:43.551841  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:33:43.551911  1019 net.cpp:165] Memory required for data: 49152
I0605 09:33:43.551952  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:33:43.552073  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:33:43.552121  1019 net.cpp:434] Convolution1 <- data
I0605 09:33:43.552183  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:33:43.552625  1019 net.cpp:150] Setting up Convolution1
I0605 09:33:43.552728  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:33:43.552791  1019 net.cpp:165] Memory required for data: 337152
I0605 09:33:43.552908  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:33:43.553004  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:33:43.553042  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:33:43.553107  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:33:43.553252  1019 net.cpp:150] Setting up Pooling1
I0605 09:33:43.553292  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:33:43.553349  1019 net.cpp:165] Memory required for data: 409152
I0605 09:33:43.553377  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:33:43.553468  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:33:43.553501  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:33:43.553558  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:33:43.555176  1019 net.cpp:150] Setting up Convolution2
I0605 09:33:43.555330  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:33:43.555397  1019 net.cpp:165] Memory required for data: 544352
I0605 09:33:43.555502  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:33:43.555596  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:33:43.555634  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:33:43.555697  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:33:43.555855  1019 net.cpp:150] Setting up Pooling2
I0605 09:33:43.555891  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:33:43.555944  1019 net.cpp:165] Memory required for data: 578152
I0605 09:33:43.556063  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:33:43.556190  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:33:43.556231  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:33:43.556296  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:33:43.570938  1019 net.cpp:150] Setting up Convolution3
I0605 09:33:43.571097  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:33:43.571182  1019 net.cpp:165] Memory required for data: 610552
I0605 09:33:43.571298  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:33:43.571481  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:33:43.571938  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:33:43.572074  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:33:43.572299  1019 net.cpp:150] Setting up Pooling3
I0605 09:33:43.572345  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:33:43.572656  1019 net.cpp:165] Memory required for data: 620552
I0605 09:33:43.572705  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:33:43.572819  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:33:43.572849  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:33:43.572912  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:33:43.644387  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:33:43.644580  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:33:43.644640  1019 net.cpp:165] Memory required for data: 622552
I0605 09:33:43.644718  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:33:43.644805  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:33:43.644837  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:33:43.644892  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:33:43.644971  1019 net.cpp:150] Setting up ReLU1
I0605 09:33:43.644997  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:33:43.645030  1019 net.cpp:165] Memory required for data: 624552
I0605 09:33:43.645052  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:33:43.645102  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:33:43.645125  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:33:43.645162  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:33:43.645644  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:33:43.645704  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:33:43.645753  1019 net.cpp:165] Memory required for data: 624604
I0605 09:33:43.645830  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:33:43.645889  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:33:43.645915  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:33:43.645954  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:33:43.646121  1019 net.cpp:150] Setting up Softmax1
I0605 09:33:43.646145  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:33:43.646178  1019 net.cpp:165] Memory required for data: 624656
I0605 09:33:43.646203  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:33:43.646227  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:33:43.646248  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:33:43.646270  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:33:43.646289  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:33:43.646309  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:33:43.646332  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:33:43.646353  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:33:43.646375  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:33:43.646396  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:33:43.646417  1019 net.cpp:228] input does not need backward computation.
I0605 09:33:43.646435  1019 net.cpp:270] This network produces output Softmax1
I0605 09:33:43.646479  1019 net.cpp:283] Network initialization done.
I0605 09:33:43.688169  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:33:43.701076  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:35:11.037279  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:35:11.037477  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:35:11.037559  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:35:11.038940  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:35:11.039052  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:35:11.039074  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:35:11.039291  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:35:11.040084  1019 layer_factory.hpp:77] Creating layer input
I0605 09:35:11.040159  1019 net.cpp:100] Creating Layer input
I0605 09:35:11.040190  1019 net.cpp:408] input -> data
I0605 09:35:11.040277  1019 net.cpp:150] Setting up input
I0605 09:35:11.040299  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:35:11.040338  1019 net.cpp:165] Memory required for data: 49152
I0605 09:35:11.040395  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:35:11.040449  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:35:11.040472  1019 net.cpp:434] Convolution1 <- data
I0605 09:35:11.040529  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:35:11.040788  1019 net.cpp:150] Setting up Convolution1
I0605 09:35:11.040818  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:35:11.040856  1019 net.cpp:165] Memory required for data: 337152
I0605 09:35:11.040915  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:35:11.040959  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:35:11.040982  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:35:11.041021  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:35:11.041083  1019 net.cpp:150] Setting up Pooling1
I0605 09:35:11.041105  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:35:11.041144  1019 net.cpp:165] Memory required for data: 409152
I0605 09:35:11.041164  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:35:11.041210  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:35:11.041231  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:35:11.041276  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:35:11.043053  1019 net.cpp:150] Setting up Convolution2
I0605 09:35:11.043284  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:35:11.043354  1019 net.cpp:165] Memory required for data: 544352
I0605 09:35:11.043567  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:35:11.043690  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:35:11.043730  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:35:11.043794  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:35:11.043936  1019 net.cpp:150] Setting up Pooling2
I0605 09:35:11.043965  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:35:11.044020  1019 net.cpp:165] Memory required for data: 578152
I0605 09:35:11.044049  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:35:11.044131  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:35:11.044159  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:35:11.044201  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:35:11.052757  1019 net.cpp:150] Setting up Convolution3
I0605 09:35:11.052943  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:35:11.053037  1019 net.cpp:165] Memory required for data: 610552
I0605 09:35:11.053133  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:35:11.053184  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:35:11.053215  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:35:11.053259  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:35:11.053337  1019 net.cpp:150] Setting up Pooling3
I0605 09:35:11.053357  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:35:11.053436  1019 net.cpp:165] Memory required for data: 620552
I0605 09:35:11.053457  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:35:11.053526  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:35:11.053547  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:35:11.053577  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:35:11.132138  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:35:11.133778  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:35:11.135146  1019 net.cpp:165] Memory required for data: 622552
I0605 09:35:11.136108  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:35:11.137181  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:35:11.138114  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:35:11.139057  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:35:11.140399  1019 net.cpp:150] Setting up ReLU1
I0605 09:35:11.141422  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:35:11.141525  1019 net.cpp:165] Memory required for data: 624552
I0605 09:35:11.141559  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:35:11.141626  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:35:11.141654  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:35:11.141700  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:35:11.142164  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:35:11.142215  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:35:11.142257  1019 net.cpp:165] Memory required for data: 624604
I0605 09:35:11.142321  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:35:11.142370  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:35:11.142395  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:35:11.142436  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:35:11.142506  1019 net.cpp:150] Setting up Softmax1
I0605 09:35:11.142529  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:35:11.142557  1019 net.cpp:165] Memory required for data: 624656
I0605 09:35:11.142580  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:35:11.142604  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:35:11.142627  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:35:11.142647  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:35:11.142666  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:35:11.142688  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:35:11.142712  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:35:11.142736  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:35:11.142762  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:35:11.142789  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:35:11.142810  1019 net.cpp:228] input does not need backward computation.
I0605 09:35:11.142837  1019 net.cpp:270] This network produces output Softmax1
I0605 09:35:11.142889  1019 net.cpp:283] Network initialization done.
I0605 09:35:11.181095  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:35:11.190171  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
W0605 09:36:33.449350  1019 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface
W0605 09:36:33.449707  1019 _caffe.cpp:123] Use this instead (with the named "weights" parameter):
W0605 09:36:33.449755  1019 _caffe.cpp:125] Net('/home/pi/Desktop/trained_model_64/deploy.prototxt', 1, weights='/home/pi/Desktop/trained_model_64/lenet_iter_2070.caffemodel')
I0605 09:36:33.451195  1019 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/pi/Desktop/trained_model_64/deploy.prototxt
I0605 09:36:33.451310  1019 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0605 09:36:33.451331  1019 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0605 09:36:33.451658  1019 net.cpp:58] Initializing net from parameters: 
name: "Lenet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 64
      dim: 64
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  convolution_param {
    num_output: 100
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "Softmax1"
  type: "Softmax"
  bottom: "InnerProduct2"
  top: "Softmax1"
}
I0605 09:36:33.452592  1019 layer_factory.hpp:77] Creating layer input
I0605 09:36:33.452807  1019 net.cpp:100] Creating Layer input
I0605 09:36:33.452845  1019 net.cpp:408] input -> data
I0605 09:36:33.452958  1019 net.cpp:150] Setting up input
I0605 09:36:33.452983  1019 net.cpp:157] Top shape: 1 3 64 64 (12288)
I0605 09:36:33.453039  1019 net.cpp:165] Memory required for data: 49152
I0605 09:36:33.453069  1019 layer_factory.hpp:77] Creating layer Convolution1
I0605 09:36:33.453140  1019 net.cpp:100] Creating Layer Convolution1
I0605 09:36:33.453166  1019 net.cpp:434] Convolution1 <- data
I0605 09:36:33.453207  1019 net.cpp:408] Convolution1 -> Convolution1
I0605 09:36:33.453691  1019 net.cpp:150] Setting up Convolution1
I0605 09:36:33.453773  1019 net.cpp:157] Top shape: 1 20 60 60 (72000)
I0605 09:36:33.453826  1019 net.cpp:165] Memory required for data: 337152
I0605 09:36:33.453918  1019 layer_factory.hpp:77] Creating layer Pooling1
I0605 09:36:33.453995  1019 net.cpp:100] Creating Layer Pooling1
I0605 09:36:33.454023  1019 net.cpp:434] Pooling1 <- Convolution1
I0605 09:36:33.454064  1019 net.cpp:408] Pooling1 -> Pooling1
I0605 09:36:33.454174  1019 net.cpp:150] Setting up Pooling1
I0605 09:36:33.454200  1019 net.cpp:157] Top shape: 1 20 30 30 (18000)
I0605 09:36:33.454234  1019 net.cpp:165] Memory required for data: 409152
I0605 09:36:33.454267  1019 layer_factory.hpp:77] Creating layer Convolution2
I0605 09:36:33.454336  1019 net.cpp:100] Creating Layer Convolution2
I0605 09:36:33.454430  1019 net.cpp:434] Convolution2 <- Pooling1
I0605 09:36:33.454478  1019 net.cpp:408] Convolution2 -> Convolution2
I0605 09:36:33.456573  1019 net.cpp:150] Setting up Convolution2
I0605 09:36:33.456840  1019 net.cpp:157] Top shape: 1 50 26 26 (33800)
I0605 09:36:33.456915  1019 net.cpp:165] Memory required for data: 544352
I0605 09:36:33.457015  1019 layer_factory.hpp:77] Creating layer Pooling2
I0605 09:36:33.457092  1019 net.cpp:100] Creating Layer Pooling2
I0605 09:36:33.457131  1019 net.cpp:434] Pooling2 <- Convolution2
I0605 09:36:33.457180  1019 net.cpp:408] Pooling2 -> Pooling2
I0605 09:36:33.457296  1019 net.cpp:150] Setting up Pooling2
I0605 09:36:33.457324  1019 net.cpp:157] Top shape: 1 50 13 13 (8450)
I0605 09:36:33.457355  1019 net.cpp:165] Memory required for data: 578152
I0605 09:36:33.457438  1019 layer_factory.hpp:77] Creating layer Convolution3
I0605 09:36:33.457561  1019 net.cpp:100] Creating Layer Convolution3
I0605 09:36:33.457587  1019 net.cpp:434] Convolution3 <- Pooling2
I0605 09:36:33.457655  1019 net.cpp:408] Convolution3 -> Convolution3
I0605 09:36:33.466274  1019 net.cpp:150] Setting up Convolution3
I0605 09:36:33.466570  1019 net.cpp:157] Top shape: 1 100 9 9 (8100)
I0605 09:36:33.466688  1019 net.cpp:165] Memory required for data: 610552
I0605 09:36:33.466840  1019 layer_factory.hpp:77] Creating layer Pooling3
I0605 09:36:33.466953  1019 net.cpp:100] Creating Layer Pooling3
I0605 09:36:33.467000  1019 net.cpp:434] Pooling3 <- Convolution3
I0605 09:36:33.467072  1019 net.cpp:408] Pooling3 -> Pooling3
I0605 09:36:33.467253  1019 net.cpp:150] Setting up Pooling3
I0605 09:36:33.467284  1019 net.cpp:157] Top shape: 1 100 5 5 (2500)
I0605 09:36:33.467339  1019 net.cpp:165] Memory required for data: 620552
I0605 09:36:33.467470  1019 layer_factory.hpp:77] Creating layer InnerProduct1
I0605 09:36:33.467669  1019 net.cpp:100] Creating Layer InnerProduct1
I0605 09:36:33.467710  1019 net.cpp:434] InnerProduct1 <- Pooling3
I0605 09:36:33.467782  1019 net.cpp:408] InnerProduct1 -> InnerProduct1
I0605 09:36:33.547677  1019 net.cpp:150] Setting up InnerProduct1
I0605 09:36:33.547797  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:36:33.547843  1019 net.cpp:165] Memory required for data: 622552
I0605 09:36:33.547920  1019 layer_factory.hpp:77] Creating layer ReLU1
I0605 09:36:33.547981  1019 net.cpp:100] Creating Layer ReLU1
I0605 09:36:33.548020  1019 net.cpp:434] ReLU1 <- InnerProduct1
I0605 09:36:33.548064  1019 net.cpp:395] ReLU1 -> InnerProduct1 (in-place)
I0605 09:36:33.548254  1019 net.cpp:150] Setting up ReLU1
I0605 09:36:33.548279  1019 net.cpp:157] Top shape: 1 500 (500)
I0605 09:36:33.548306  1019 net.cpp:165] Memory required for data: 624552
I0605 09:36:33.548328  1019 layer_factory.hpp:77] Creating layer InnerProduct2
I0605 09:36:33.548398  1019 net.cpp:100] Creating Layer InnerProduct2
I0605 09:36:33.548419  1019 net.cpp:434] InnerProduct2 <- InnerProduct1
I0605 09:36:33.548454  1019 net.cpp:408] InnerProduct2 -> InnerProduct2
I0605 09:36:33.548981  1019 net.cpp:150] Setting up InnerProduct2
I0605 09:36:33.549019  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:36:33.549051  1019 net.cpp:165] Memory required for data: 624604
I0605 09:36:33.549106  1019 layer_factory.hpp:77] Creating layer Softmax1
I0605 09:36:33.549147  1019 net.cpp:100] Creating Layer Softmax1
I0605 09:36:33.549170  1019 net.cpp:434] Softmax1 <- InnerProduct2
I0605 09:36:33.549199  1019 net.cpp:408] Softmax1 -> Softmax1
I0605 09:36:33.549252  1019 net.cpp:150] Setting up Softmax1
I0605 09:36:33.549273  1019 net.cpp:157] Top shape: 1 13 (13)
I0605 09:36:33.549299  1019 net.cpp:165] Memory required for data: 624656
I0605 09:36:33.549321  1019 net.cpp:228] Softmax1 does not need backward computation.
I0605 09:36:33.549345  1019 net.cpp:228] InnerProduct2 does not need backward computation.
I0605 09:36:33.549391  1019 net.cpp:228] ReLU1 does not need backward computation.
I0605 09:36:33.549410  1019 net.cpp:228] InnerProduct1 does not need backward computation.
I0605 09:36:33.549432  1019 net.cpp:228] Pooling3 does not need backward computation.
I0605 09:36:33.549451  1019 net.cpp:228] Convolution3 does not need backward computation.
I0605 09:36:33.549471  1019 net.cpp:228] Pooling2 does not need backward computation.
I0605 09:36:33.549520  1019 net.cpp:228] Convolution2 does not need backward computation.
I0605 09:36:33.549540  1019 net.cpp:228] Pooling1 does not need backward computation.
I0605 09:36:33.549561  1019 net.cpp:228] Convolution1 does not need backward computation.
I0605 09:36:33.549583  1019 net.cpp:228] input does not need backward computation.
I0605 09:36:33.549603  1019 net.cpp:270] This network produces output Softmax1
I0605 09:36:33.549650  1019 net.cpp:283] Network initialization done.
I0605 09:36:33.584983  1019 net.cpp:761] Ignoring source layer ImageData1
I0605 09:36:33.595674  1019 net.cpp:761] Ignoring source layer SoftmaxWithLoss1
VIDIOC_QUERYCTRL: Input/output error
VIDIOC_QUERYCTRL: Input/output error
